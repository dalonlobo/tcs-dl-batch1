{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This demonstrates the following on imdb data\n",
    "Use GloVe model for embedding (without learning) instead of keras embedding (that leanrs)\n",
    "1) Create a LSTM model with return_sequence = False for the last layer\n",
    "2) Learn a WordEmbedding using word2vec (download weights, and train using additional sample sentences) using both skipgram and CBoW\n",
    "3) Predict sample sentences using both approach 1 and 2\n",
    "4) Do some analogies calculus using glove and word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import Dense,  Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('IMDB dataset  {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---Features: review in number sequence After padding---')\n",
    "print(X_train[6])\n",
    "print('Sentiment 1 = positve, 0 = negative')\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2id = imdb.get_word_index() \n",
    "id2word = {i: word  for word, i in w2id.items()}\n",
    "print (len(w2id), id2word[1], w2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jibberish data?\n",
    "actual_data = []\n",
    "for i in range (len(X_train[6])):\n",
    "    ind = X_train[6][i]\n",
    "    if(ind != 0):\n",
    "        actual_data.append(id2word[ind])\n",
    "actual_data = \" \".join (actual_data )\n",
    "print (actual_data, \"\\nAnd sentiment is \", y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id ={w: i+3 for w, i in w2id.items()}\n",
    "word2id[\"__PADDING__\"] = 0\n",
    "word2id[\"__START__\"] = 1\n",
    "word2id[\"__UNK__\"] = 2\n",
    "\n",
    "#This returns the index of the words from 1 to n with 1 being the most frequently occuring word, \n",
    "\n",
    "# and n the least frequently occuring word\n",
    "\n",
    "print (type(word2id), len (word2id))\n",
    "\n",
    "id2word = {i: word  for word, i in word2id.items()}\n",
    "print(id2word[1], id2word[2], #id2word[3],#\n",
    "      id2word[4], id2word[5], id2word[6], id2word[88586])\n",
    "actual_data = []\n",
    "for i in range (len(X_train[6])):\n",
    "    ind = X_train[6][i]\n",
    "    if(ind != 0):\n",
    "        actual_data.append(id2word[ind])\n",
    "actual_data = \" \".join (actual_data )\n",
    "print (actual_data, \"\\nAnd sentiment is \", y_train[6])\n",
    "print (word2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 2697\n"
     ]
    }
   ],
   "source": [
    "print('Maximum review length: {}'.format(\n",
    "len((max((X_train + X_test), key=len)))))\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_data(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
    "\n",
    "        word_to_vec_map = {}\n",
    "        curr_word=None\n",
    "        WordToInd = {}\n",
    "        IndToWord = {}\n",
    "        i = -1\n",
    "        try:\n",
    "            for line in f:\n",
    "                i+=1\n",
    "                try:\n",
    "                    line = line.strip().split()\n",
    "                    curr_word = line[0]                   \n",
    "                   \n",
    "                    word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "                    WordToInd[curr_word] = i\n",
    "                    IndToWord[i] = curr_word\n",
    "                except Exception as E:\n",
    "                    print (\"got An exception, word=\", curr_word, i)\n",
    "                    pass         \n",
    "        except Exception as E:\n",
    "            print (\"got An exception before for, word=\", curr_word, i)\n",
    "            pass         \n",
    "            \n",
    "    return word_to_vec_map, WordToInd, IndToWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_map, WordToInd, IndToWord = read_glove_data('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now create an embedding vector\n",
    "gl_embed = np.zeros((vocabulary_size, 50))\n",
    "for i, word in id2word.items():\n",
    "    if i >= vocabulary_size:\n",
    "        continue\n",
    "    embed = word_to_vec_map.get(word)\n",
    "    if embed is not None:\n",
    "        gl_embed[i] = embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 50)\n",
      "[ 0.90754   -0.38322    0.67648   -0.20222    0.15156    0.13627\n",
      " -0.48813    0.48223   -0.095715   0.18306    0.27007    0.41415\n",
      " -0.48933   -0.0076005  0.79662    1.0989     0.53802   -0.54468\n",
      " -0.16063   -0.98348   -0.19188   -0.2144     0.19959   -0.31341\n",
      "  0.24101   -2.2662    -0.25926   -0.10898    0.66177   -0.48104\n",
      "  3.6298     0.45397   -0.64484   -0.52244    0.042922  -0.16605\n",
      "  0.097102   0.044836   0.20389   -0.46322   -0.46434    0.32394\n",
      "  0.25984    0.40849    0.20351    0.058722  -0.16408    0.20672\n",
      " -0.1844     0.071147 ] [-0.17168   0.3855    0.50195  -0.58935  -1.0504    0.31327   0.12463\n",
      " -1.36     -1.2506   -0.94656   0.28644   1.2532   -0.54125  -0.25494\n",
      " -0.69635   0.15366  -0.9228    0.70842  -0.38708   0.45516  -0.26158\n",
      " -0.82918   0.45284   0.46074  -0.60629   0.31825  -0.56607   0.19254\n",
      " -0.22289   0.15175   0.37615   0.15187   0.78425   1.1723    0.44189\n",
      " -0.17558   0.83528  -0.76923  -0.19997   0.50355   0.70561  -0.31922\n",
      "  0.20774   0.030179 -0.31608  -0.20886   0.020144 -0.19314   0.60182\n",
      "  0.71473 ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print (gl_embed.shape)\n",
    "print (gl_embed[100], gl_embed[10], gl_embed[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "modelLSTM=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_epochs = 3\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 50)           500000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 591,777\n",
      "Trainable params: 91,777\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Lets now create an LSTM model with embedding layer.\n",
    "#Set the embedding layer as not trainable, and sets its weights\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras import Sequential\n",
    "embedding_size=50\n",
    "modelLSTM=Sequential()\n",
    "modelLSTM.add(Embedding(vocabulary_size, embedding_size, input_length=max_words,\n",
    "                       weights=[gl_embed], trainable=False))\n",
    "modelLSTM.add(LSTM(128, dropout=0.2, return_sequences=False)) \n",
    "modelLSTM.add(Dense(1, activation='sigmoid'))\n",
    "print(modelLSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24744 samples, validate on 256 samples\n",
      "Epoch 1/3\n",
      "24744/24744 [==============================] - 602s 24ms/step - loss: 0.6393 - acc: 0.6303 - val_loss: 0.5644 - val_acc: 0.7148\n",
      "Epoch 2/3\n",
      "24744/24744 [==============================] - 592s 24ms/step - loss: 0.5972 - acc: 0.6864 - val_loss: 0.4757 - val_acc: 0.8047\n",
      "Epoch 3/3\n",
      "24744/24744 [==============================] - 585s 24ms/step - loss: 0.5668 - acc: 0.7096 - val_loss: 0.4632 - val_acc: 0.8047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f98e207d68>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLSTM.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.75972\n"
     ]
    }
   ],
   "source": [
    "scores = modelLSTM.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets now create an LSTM model with embedding layer.\n",
    "#Set the embedding layer as trainable, and sets its weights\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras import Sequential\n",
    "embedding_size=50\n",
    "modelLSTM1=Sequential()\n",
    "modelLSTM1.add(Embedding(vocabulary_size, embedding_size, input_length=max_words,\n",
    "                       weights=[gl_embed]))\n",
    "modelLSTM1.add(LSTM(128, dropout=0.2, return_sequences=False)) \n",
    "modelLSTM1.add(Dense(1, activation='sigmoid'))\n",
    "print(modelLSTM1.summary())\n",
    "modelLSTM1.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM1.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets now create an LSTM model with keras embedding layer and let it learn.\n",
    "#Set the embedding layer as not trainable, and sets its weights\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras import Sequential\n",
    "embedding_size=50\n",
    "modelLSTM2=Sequential()\n",
    "modelLSTM2.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "modelLSTM2.add(LSTM(128, dropout=0.2, return_sequences=False)) \n",
    "modelLSTM2.add(Dense(1, activation='sigmoid'))\n",
    "print(modelLSTM2.summary())\n",
    "modelLSTM2.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM2.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now going to predict these reviews\n"
     ]
    }
   ],
   "source": [
    "reviewList = [\"the movie was not at all bad\", \n",
    "              \"the movie was a total waste of my time\",\n",
    "              \"the food was so deliciously delicious that i felt sinfully wicked\" ,   \n",
    "              \"the food was so wicked that after eating i felt sinful\"\n",
    "             ]\n",
    "print (\"Now going to predict these reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (word2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review= the movie was not at all bad\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= not id= 24\n",
      "word= at id= 33\n",
      "word= all id= 32\n",
      "word= bad id= 78\n",
      "Prediction Probability for  \" the movie was not at all bad \" LSTM  =  0.3426385 Sentiment= Negative \n",
      "\n",
      "review= the movie was a total waste of my time\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= a id= 6\n",
      "word= total id= 964\n",
      "word= waste id= 437\n",
      "word= of id= 7\n",
      "word= my id= 61\n",
      "word= time id= 58\n",
      "Prediction Probability for  \" the movie was a total waste of my time \" LSTM  =  0.3150782 Sentiment= Negative \n",
      "\n",
      "review= the food was so deliciously delicious that i felt sinfully wicked\n",
      "word= the id= 4\n",
      "word= food id= 1644\n",
      "word= was id= 16\n",
      "word= so id= 38\n",
      "word= deliciously id= 6922\n",
      "word= delicious id= 6335\n",
      "word= that id= 15\n",
      "word= i id= 13\n",
      "word= felt id= 421\n",
      "sinfully Appended 2\n",
      "word= wicked id= 3799\n",
      "Prediction Probability for  \" the food was so deliciously delicious that i felt sinfully wicked \" LSTM  =  0.46510503 Sentiment= Negative \n",
      "\n",
      "review= the food was so wicked that after eating i felt sinful\n",
      "word= the id= 4\n",
      "word= food id= 1644\n",
      "word= was id= 16\n",
      "word= so id= 38\n",
      "word= wicked id= 3799\n",
      "word= that id= 15\n",
      "word= after id= 103\n",
      "word= eating id= 1889\n",
      "word= i id= 13\n",
      "word= felt id= 421\n",
      "got a word outside teh vocab_index sinful 17010 breaking\n",
      "Prediction Probability for  \" the food was so wicked that after eating i felt sinful \" LSTM  =  0.33175677 Sentiment= Negative \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def PredictSentiment(reviewList, ModelListTuple):\n",
    "    sentiment= {True: \"Positive\",\n",
    "               False: \"Negative\"}\n",
    "    Threshold = 0.5\n",
    "    for r in reviewList:\n",
    "        words = r.split()\n",
    "        review = []\n",
    "        print (\"review=\", r)\n",
    "        for word in words:\n",
    "          if word not in word2id: \n",
    "            review.append(2)\n",
    "            print (word, \"Appended 2\")\n",
    "          else:\n",
    "            if (word2id[word]) >= vocabulary_size:\n",
    "                print(\"got a word outside teh vocab_index\", word, word2id[word], \"breaking\")\n",
    "                break\n",
    "            print (\"word=\", word, \"id=\", word2id[word])\n",
    "            review.append(word2id[word]) \n",
    "        review = keras.preprocessing.sequence.pad_sequences([review],\n",
    "          truncating='pre', padding='pre', maxlen=max_words)\n",
    "        for i,m in enumerate(ModelListTuple):\n",
    "            if m[0] is not None:\n",
    "                prediction = m[0].predict(review)\n",
    "                print(\"Prediction Probability for \", \"\\\"\",r, \"\\\"\",ModelListTuple[i][1],\" = \", prediction[0][0], \"Sentiment=\", \n",
    "                      sentiment[prediction[0][0]>Threshold], \"\\n\")\n",
    "                \n",
    "PredictSentiment(reviewList,[ (modelLSTM, \"LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "model = modelCNN\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "i = 0\n",
    "print (\"#Layers = \", len(layer_outputs))\n",
    "print (type(model.layers[0]))\n",
    "l = model.layers[0]\n",
    "print (len(l.get_weights()))\n",
    "print (l.get_weights()[0].shape)\n",
    "print (word2id[\"apple\"], word2id[\"orange\"], id2word[6335], id2word[6922])\n",
    "print (l.get_weights()[0][word2id[\"apple\"]])\n",
    "print (l.get_weights()[0][word2id[\"orange\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now use patience, and early stopping\n",
    "#early1 = keras.callbacks.EarlyStopping(monitor =\"val_acc\", patience=1, restore_best_weights=True)\n",
    "#early1 = keras.callbacks.EarlyStopping(monitor =\"acc\", patience=2, baseline = 0.95)\n",
    "#early1 = keras.callbacks.EarlyStopping(monitor =\"val_acc\", patience=1, restore_best_weights=True)\n",
    "\n",
    "callback_list = [#early1\n",
    "                keras.callbacks.ModelCheckpoint(filepath=\"my_mod2.h5\", monitor=\"val_acc\",\n",
    "                                               save_best_only=True),\n",
    "                #keras.callbacks.TerminateOnNaN()\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3 = X_train2[:1000]\n",
    "Y_train3 = y_train2[:1000]\n",
    "print (X_train3.shape, Y_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train3, Y_train3, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=15, #num_epochs,\n",
    "             callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
