{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook will make a Word2Vec embedding using CBoW method\n",
    "#Training corpus = 4 sentences\n",
    "#For each sentence form input, output with window of 1\n",
    "#Logic Read the Training corpus (having grand total of 4 sentences)\n",
    "#Form a wordToIndex map : {word:  index}, and IndexToWord map {index:word}\n",
    "#Use the WordToIndex map to form a OHE\n",
    "# #of Dimensions = 6\n",
    "#For each sentence do:\n",
    "    # extract the context and target pair (also same as input, output)\n",
    "    # Convert them to OHE\n",
    "    # Add them to x_train and y_train\n",
    "# Feed the x_train and y_train to the Keras NN for an epoch of 100\n",
    "# At the the end extract the weights of hidden layer. That will act as word embedding    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "in linguistics word embeddings were discussed in the research area of distributional semantics\n",
      " the area developed gradually and really took off after 2010, partly because important advances had been made since then on the quality of vectors and the training speed of the model\n",
      " the results presented by asgari and mofrad suggest that biovectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns\n"
     ]
    }
   ],
   "source": [
    "f = open(\"w2v.txt\")\n",
    "text =f.read()\n",
    "text = text.lower()\n",
    "training_corpus = text.split(\".\")\n",
    "print (len(training_corpus))\n",
    "for i in range(0, len(training_corpus), 10):\n",
    "    print (training_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras import Sequential\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = [\n",
    "    \"Today is a great day filled with blue sky EOS\",\n",
    "    \"I am here to make my day EOS\",\n",
    "    \"I need to fill up the gas for the long drive EOS\",\n",
    "    \"Love driving in clear skies EOS\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in linguistics word embeddings were discussed in the research area of distributional semantics\n",
      " it aims to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data\n",
      " the underlying idea that \"a word is characterized by the company it keeps\" was popularized by firth\n",
      "\n",
      "\n",
      "the technique of representing words as vectors has roots in the 1960s with the development of the vector space model for information retrieval\n",
      " reducing the number of dimensions using singular value decomposition then led to the introduction of latent semantic analysis in the late 1980s\n",
      "in 2000 bengio et al\n",
      " provided in a series of papers the \"neural probabilistic language models\" to reduce the high dimensionality of words representations in contexts by \"learning a distributed representation for words\"\n",
      " (bengio et al, 2003)\n",
      " word embeddings come in two different styles, one in which words are expressed as vectors of co-occurring words, and another in which words are expressed as vectors of linguistic contexts in which the words occur; these different styles are studied in (lavelli et al, 2004)\n",
      " roweis and saul published in science how to use \"locally linear embedding\" (lle) to discover representations of high dimensional data structures\n",
      " the area developed gradually and really took off after 2010, partly because important advances had been made since then on the quality of vectors and the training speed of the model\n",
      "\n",
      "\n",
      "there are many branches and many research groups working on word embeddings\n",
      " in 2013, a team at google led by tomas mikolov created word2vec, a word embedding toolkit which can train vector space models faster than the previous approaches\n",
      " most new word embedding techniques rely on a neural network architecture instead of more traditional n-gram models and unsupervised learning\n",
      "\n",
      "\n",
      "limitations\n",
      "one of the main limitations of word embeddings (word vector space models in general) is that possible meanings of a word are conflated into a single representation (a single vector in the semantic space)\n",
      " sense embeddings are a solution to this problem: individual meanings of words are represented as distinct vectors in the space\n",
      "\n",
      "\n",
      "for biological sequences: biovectors\n",
      "word embeddings for n-grams in biological sequences (e\n",
      "g\n",
      " dna, rna, and proteins) for bioinformatics applications have been proposed by asgari and mofrad\n",
      " named bio-vectors (biovec) to refer to biological sequences in general with protein-vectors (protvec) for proteins (amino-acid sequences) and gene-vectors (genevec) for gene sequences, this representation can be widely used in applications of deep learning in proteomics and genomics\n",
      " the results presented by asgari and mofrad suggest that biovectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns\n",
      "\n",
      "\n",
      "thought vectors\n",
      "thought vectors are an extension of word embeddings to entire sentences or even documents\n",
      " some researchers hope that these can improve the quality of machine translation\n",
      "\n",
      "\n",
      "software\n",
      "software for training and using word embeddings includes tomas mikolov's word2vec, stanford university's glove, allennlp's elmo,fasttext, gensim, indra and deeplearning4j\n",
      " principal component analysis (pca) and t-distributed stochastic neighbour embedding (t-sne) are both used to reduce the dimensionality of word vector spaces and visualize word embeddings and clusters\n",
      "\n",
      "\n",
      "examples of application\n",
      "for instance, the fasttext is also used to calculate word embeddings for text corpora in sketch engine that are available online\n",
      "\n",
      "290\n"
     ]
    }
   ],
   "source": [
    "def FormUniqueIndexAndVectorizeInput(tc):\n",
    "    WordToInd = {}\n",
    "    IndToWord = {}\n",
    "    vect_tc = []    \n",
    "    ind = 0\n",
    "    for sent in tc:\n",
    "        print (sent)\n",
    "        words = sent.split()\n",
    "        ind_tc= []\n",
    "        for s in words:\n",
    "            if s not in WordToInd:\n",
    "                WordToInd[s] = ind\n",
    "                IndToWord[ind] = s\n",
    "                ind += 1\n",
    "            ind_tc.append(WordToInd[s])\n",
    "        vect_tc.append(ind_tc)\n",
    "    return WordToInd, IndToWord, vect_tc\n",
    "\n",
    "WordToInd, IndToWord, vect_tc = FormUniqueIndexAndVectorizeInput(training_corpus)\n",
    "print (len(WordToInd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def CBoWInput(vtc, window=2):\n",
    "    train = []\n",
    "    for sent in vtc:\n",
    "        for i,word in enumerate(sent):\n",
    "            if (i < len(sent) -1):\n",
    "                j = 0\n",
    "                while j< window:\n",
    "                    train.append ((sent[i-j], sent[i+1]))\n",
    "                    j += 1\n",
    "                    if(i - j < 0):\n",
    "                        break\n",
    "        #print (\"for sentence=\", sent, \"training set=\",train)\n",
    "    x_train = np.ndarray((len(train), 1))\n",
    "    y_train = np.ndarray((len(train), 1))\n",
    "    for i,t in enumerate(train):\n",
    "        x_train[i] = t[0]\n",
    "        y_train[i] = t[1]\n",
    "\n",
    "    return train, x_train, y_train\n",
    "training_data, x, y = CBoWInput(vect_tc)\n",
    "#for t in training_data:\n",
    " #   print (IndToWord[t[0]], IndToWord[t[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(993, 1)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 6)                 1746      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 290)               2030      \n",
      "=================================================================\n",
      "Total params: 3,776\n",
      "Trainable params: 3,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#create Model\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras import Sequential\n",
    "\n",
    "modelNN=Sequential()\n",
    "\n",
    "modelNN.add(Dense(6, input_shape=(len(WordToInd),), activation = \"relu\"))\n",
    "modelNN.add(Dense(len(WordToInd), activation='softmax'))\n",
    "modelNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(modelNN.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "batch_size=28\n",
    "num_epochs = 500\n",
    "x_enc = to_categorical(x, num_classes=len(WordToInd))\n",
    "y_enc = to_categorical(y, num_classes=len(WordToInd))\n",
    "#print (x_enc.shape, y_enc.shape)\n",
    "#print (x_enc[0], x_enc[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "993/993 [==============================] - 0s 146us/step - loss: 2.7800 - acc: 0.2800\n",
      "Epoch 2/500\n",
      "993/993 [==============================] - 0s 137us/step - loss: 2.7780 - acc: 0.2769\n",
      "Epoch 3/500\n",
      "993/993 [==============================] - 0s 143us/step - loss: 2.7745 - acc: 0.2820\n",
      "Epoch 4/500\n",
      "993/993 [==============================] - 0s 150us/step - loss: 2.7715 - acc: 0.2840\n",
      "Epoch 5/500\n",
      "993/993 [==============================] - 0s 147us/step - loss: 2.7686 - acc: 0.2830\n",
      "Epoch 6/500\n",
      "993/993 [==============================] - 0s 157us/step - loss: 2.7662 - acc: 0.2810\n",
      "Epoch 7/500\n",
      "993/993 [==============================] - 0s 150us/step - loss: 2.7628 - acc: 0.2820\n",
      "Epoch 8/500\n",
      "993/993 [==============================] - 0s 148us/step - loss: 2.7601 - acc: 0.2830\n",
      "Epoch 9/500\n",
      "993/993 [==============================] - 0s 150us/step - loss: 2.7572 - acc: 0.2870\n",
      "Epoch 10/500\n",
      "993/993 [==============================] - 0s 150us/step - loss: 2.7551 - acc: 0.2820\n",
      "Epoch 11/500\n",
      "993/993 [==============================] - 0s 150us/step - loss: 2.7528 - acc: 0.2860\n",
      "Epoch 12/500\n",
      "993/993 [==============================] - 0s 144us/step - loss: 2.7501 - acc: 0.2850\n",
      "Epoch 13/500\n",
      "993/993 [==============================] - 0s 168us/step - loss: 2.7471 - acc: 0.2890\n",
      "Epoch 14/500\n",
      "993/993 [==============================] - 0s 135us/step - loss: 2.7447 - acc: 0.2900\n",
      "Epoch 15/500\n",
      "993/993 [==============================] - 0s 151us/step - loss: 2.7426 - acc: 0.2870\n",
      "Epoch 16/500\n",
      "993/993 [==============================] - 0s 149us/step - loss: 2.7384 - acc: 0.2860\n",
      "Epoch 17/500\n",
      "993/993 [==============================] - 0s 148us/step - loss: 2.7361 - acc: 0.2850\n",
      "Epoch 18/500\n",
      "993/993 [==============================] - 0s 152us/step - loss: 2.7340 - acc: 0.2870\n",
      "Epoch 19/500\n",
      "993/993 [==============================] - 0s 161us/step - loss: 2.7317 - acc: 0.2840\n",
      "Epoch 20/500\n",
      "993/993 [==============================] - 0s 197us/step - loss: 2.7279 - acc: 0.2870\n",
      "Epoch 21/500\n",
      "993/993 [==============================] - 0s 159us/step - loss: 2.7265 - acc: 0.2840\n",
      "Epoch 22/500\n",
      "993/993 [==============================] - 0s 190us/step - loss: 2.7236 - acc: 0.2860\n",
      "Epoch 23/500\n",
      "993/993 [==============================] - 0s 195us/step - loss: 2.7207 - acc: 0.2910\n",
      "Epoch 24/500\n",
      "993/993 [==============================] - 0s 177us/step - loss: 2.7193 - acc: 0.2860\n",
      "Epoch 25/500\n",
      "993/993 [==============================] - 0s 173us/step - loss: 2.7162 - acc: 0.2890\n",
      "Epoch 26/500\n",
      "993/993 [==============================] - 0s 200us/step - loss: 2.7137 - acc: 0.2860 0s - loss: 2.7437 - acc: 0.27\n",
      "Epoch 27/500\n",
      "993/993 [==============================] - 0s 217us/step - loss: 2.7107 - acc: 0.2920\n",
      "Epoch 28/500\n",
      "993/993 [==============================] - 0s 183us/step - loss: 2.7089 - acc: 0.2880\n",
      "Epoch 29/500\n",
      "993/993 [==============================] - 0s 176us/step - loss: 2.7067 - acc: 0.2880\n",
      "Epoch 30/500\n",
      "993/993 [==============================] - 0s 189us/step - loss: 2.7045 - acc: 0.2860\n",
      "Epoch 31/500\n",
      "993/993 [==============================] - 0s 198us/step - loss: 2.7021 - acc: 0.2890\n",
      "Epoch 32/500\n",
      "993/993 [==============================] - 0s 206us/step - loss: 2.6997 - acc: 0.2890\n",
      "Epoch 33/500\n",
      "993/993 [==============================] - 0s 183us/step - loss: 2.6973 - acc: 0.2870\n",
      "Epoch 34/500\n",
      "993/993 [==============================] - 0s 225us/step - loss: 2.6946 - acc: 0.2920\n",
      "Epoch 35/500\n",
      "993/993 [==============================] - 0s 216us/step - loss: 2.6924 - acc: 0.2890\n",
      "Epoch 36/500\n",
      "993/993 [==============================] - 0s 213us/step - loss: 2.6901 - acc: 0.2920\n",
      "Epoch 37/500\n",
      "993/993 [==============================] - 0s 216us/step - loss: 2.6882 - acc: 0.2870\n",
      "Epoch 38/500\n",
      "993/993 [==============================] - 0s 199us/step - loss: 2.6859 - acc: 0.2890\n",
      "Epoch 39/500\n",
      "993/993 [==============================] - 0s 161us/step - loss: 2.6835 - acc: 0.2910\n",
      "Epoch 40/500\n",
      "993/993 [==============================] - 0s 182us/step - loss: 2.6810 - acc: 0.2870\n",
      "Epoch 41/500\n",
      "993/993 [==============================] - 0s 146us/step - loss: 2.6795 - acc: 0.2890\n",
      "Epoch 42/500\n",
      "993/993 [==============================] - 0s 131us/step - loss: 2.6761 - acc: 0.2920\n",
      "Epoch 43/500\n",
      "993/993 [==============================] - 0s 131us/step - loss: 2.6745 - acc: 0.2900\n",
      "Epoch 44/500\n",
      "993/993 [==============================] - 0s 195us/step - loss: 2.6720 - acc: 0.2810\n",
      "Epoch 45/500\n",
      "993/993 [==============================] - 0s 187us/step - loss: 2.6697 - acc: 0.2890\n",
      "Epoch 46/500\n",
      "993/993 [==============================] - 0s 171us/step - loss: 2.6689 - acc: 0.2900\n",
      "Epoch 47/500\n",
      "993/993 [==============================] - 0s 147us/step - loss: 2.6654 - acc: 0.2880\n",
      "Epoch 48/500\n",
      "993/993 [==============================] - 0s 168us/step - loss: 2.6633 - acc: 0.2890\n",
      "Epoch 49/500\n",
      "993/993 [==============================] - 0s 159us/step - loss: 2.6617 - acc: 0.2880\n",
      "Epoch 50/500\n",
      "993/993 [==============================] - 0s 137us/step - loss: 2.6596 - acc: 0.2880\n",
      "Epoch 51/500\n",
      "993/993 [==============================] - 0s 140us/step - loss: 2.6574 - acc: 0.2880\n",
      "Epoch 52/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.6547 - acc: 0.2890\n",
      "Epoch 53/500\n",
      "993/993 [==============================] - 0s 121us/step - loss: 2.6540 - acc: 0.2910\n",
      "Epoch 54/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.6512 - acc: 0.2920\n",
      "Epoch 55/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.6487 - acc: 0.2880\n",
      "Epoch 56/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.6467 - acc: 0.2890\n",
      "Epoch 57/500\n",
      "993/993 [==============================] - 0s 125us/step - loss: 2.6446 - acc: 0.2890\n",
      "Epoch 58/500\n",
      "993/993 [==============================] - 0s 143us/step - loss: 2.6430 - acc: 0.2920\n",
      "Epoch 59/500\n",
      "993/993 [==============================] - 0s 140us/step - loss: 2.6411 - acc: 0.2890\n",
      "Epoch 60/500\n",
      "993/993 [==============================] - 0s 127us/step - loss: 2.6396 - acc: 0.2850\n",
      "Epoch 61/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.6380 - acc: 0.2860\n",
      "Epoch 62/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.6350 - acc: 0.2880\n",
      "Epoch 63/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.6334 - acc: 0.2890\n",
      "Epoch 64/500\n",
      "993/993 [==============================] - 0s 129us/step - loss: 2.6305 - acc: 0.2880\n",
      "Epoch 65/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.6294 - acc: 0.2860\n",
      "Epoch 66/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.6273 - acc: 0.2910\n",
      "Epoch 67/500\n",
      "993/993 [==============================] - 0s 136us/step - loss: 2.6258 - acc: 0.2910\n",
      "Epoch 68/500\n",
      "993/993 [==============================] - 0s 129us/step - loss: 2.6232 - acc: 0.2920\n",
      "Epoch 69/500\n",
      "993/993 [==============================] - 0s 127us/step - loss: 2.6216 - acc: 0.2941\n",
      "Epoch 70/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.6192 - acc: 0.2870\n",
      "Epoch 71/500\n",
      "993/993 [==============================] - 0s 125us/step - loss: 2.6173 - acc: 0.2920\n",
      "Epoch 72/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.6162 - acc: 0.2951\n",
      "Epoch 73/500\n",
      "993/993 [==============================] - 0s 130us/step - loss: 2.6134 - acc: 0.2880\n",
      "Epoch 74/500\n",
      "993/993 [==============================] - 0s 125us/step - loss: 2.6113 - acc: 0.2890\n",
      "Epoch 75/500\n",
      "993/993 [==============================] - 0s 131us/step - loss: 2.6100 - acc: 0.2961\n",
      "Epoch 76/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.6089 - acc: 0.2900\n",
      "Epoch 77/500\n",
      "993/993 [==============================] - 0s 125us/step - loss: 2.6070 - acc: 0.2880\n",
      "Epoch 78/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.6048 - acc: 0.2900\n",
      "Epoch 79/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.6031 - acc: 0.2941\n",
      "Epoch 80/500\n",
      "993/993 [==============================] - 0s 127us/step - loss: 2.6007 - acc: 0.2900\n",
      "Epoch 81/500\n",
      "993/993 [==============================] - 0s 131us/step - loss: 2.5991 - acc: 0.2910\n",
      "Epoch 82/500\n",
      "993/993 [==============================] - 0s 127us/step - loss: 2.5972 - acc: 0.2870\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993/993 [==============================] - 0s 124us/step - loss: 2.5956 - acc: 0.2860\n",
      "Epoch 84/500\n",
      "993/993 [==============================] - 0s 135us/step - loss: 2.5946 - acc: 0.2880\n",
      "Epoch 85/500\n",
      "993/993 [==============================] - 0s 135us/step - loss: 2.5925 - acc: 0.2900\n",
      "Epoch 86/500\n",
      "993/993 [==============================] - 0s 126us/step - loss: 2.5901 - acc: 0.2880\n",
      "Epoch 87/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.5887 - acc: 0.2860\n",
      "Epoch 88/500\n",
      "993/993 [==============================] - 0s 121us/step - loss: 2.5874 - acc: 0.2860\n",
      "Epoch 89/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.5852 - acc: 0.2870\n",
      "Epoch 90/500\n",
      "993/993 [==============================] - 0s 125us/step - loss: 2.5838 - acc: 0.2870\n",
      "Epoch 91/500\n",
      "993/993 [==============================] - 0s 121us/step - loss: 2.5820 - acc: 0.2931\n",
      "Epoch 92/500\n",
      "993/993 [==============================] - 0s 135us/step - loss: 2.5802 - acc: 0.2880\n",
      "Epoch 93/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.5785 - acc: 0.2910\n",
      "Epoch 94/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.5763 - acc: 0.2920\n",
      "Epoch 95/500\n",
      "993/993 [==============================] - 0s 153us/step - loss: 2.5757 - acc: 0.2880\n",
      "Epoch 96/500\n",
      "993/993 [==============================] - 0s 134us/step - loss: 2.5735 - acc: 0.2900\n",
      "Epoch 97/500\n",
      "993/993 [==============================] - 0s 139us/step - loss: 2.5713 - acc: 0.2931\n",
      "Epoch 98/500\n",
      "993/993 [==============================] - 0s 137us/step - loss: 2.5700 - acc: 0.2910\n",
      "Epoch 99/500\n",
      "993/993 [==============================] - 0s 144us/step - loss: 2.5682 - acc: 0.2910\n",
      "Epoch 100/500\n",
      "993/993 [==============================] - 0s 147us/step - loss: 2.5668 - acc: 0.2880\n",
      "Epoch 101/500\n",
      "993/993 [==============================] - 0s 131us/step - loss: 2.5657 - acc: 0.2910\n",
      "Epoch 102/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.5636 - acc: 0.2920\n",
      "Epoch 103/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.5619 - acc: 0.2860\n",
      "Epoch 104/500\n",
      "993/993 [==============================] - 0s 136us/step - loss: 2.5614 - acc: 0.2890\n",
      "Epoch 105/500\n",
      "993/993 [==============================] - 0s 138us/step - loss: 2.5587 - acc: 0.2900\n",
      "Epoch 106/500\n",
      "993/993 [==============================] - 0s 127us/step - loss: 2.5572 - acc: 0.2931\n",
      "Epoch 107/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.5569 - acc: 0.2931\n",
      "Epoch 108/500\n",
      "993/993 [==============================] - 0s 133us/step - loss: 2.5547 - acc: 0.2951\n",
      "Epoch 109/500\n",
      "993/993 [==============================] - 0s 130us/step - loss: 2.5530 - acc: 0.2890\n",
      "Epoch 110/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.5509 - acc: 0.2920\n",
      "Epoch 111/500\n",
      "993/993 [==============================] - 0s 126us/step - loss: 2.5499 - acc: 0.2910\n",
      "Epoch 112/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.5490 - acc: 0.2890\n",
      "Epoch 113/500\n",
      "993/993 [==============================] - 0s 127us/step - loss: 2.5462 - acc: 0.2890\n",
      "Epoch 114/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.5453 - acc: 0.2961\n",
      "Epoch 115/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.5433 - acc: 0.2900\n",
      "Epoch 116/500\n",
      "993/993 [==============================] - 0s 121us/step - loss: 2.5421 - acc: 0.2900\n",
      "Epoch 117/500\n",
      "993/993 [==============================] - 0s 136us/step - loss: 2.5405 - acc: 0.2941\n",
      "Epoch 118/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.5395 - acc: 0.2931\n",
      "Epoch 119/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.5377 - acc: 0.2941\n",
      "Epoch 120/500\n",
      "993/993 [==============================] - 0s 126us/step - loss: 2.5358 - acc: 0.2910\n",
      "Epoch 121/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.5347 - acc: 0.2920\n",
      "Epoch 122/500\n",
      "993/993 [==============================] - 0s 126us/step - loss: 2.5343 - acc: 0.2941\n",
      "Epoch 123/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.5328 - acc: 0.2961\n",
      "Epoch 124/500\n",
      "993/993 [==============================] - 0s 134us/step - loss: 2.5314 - acc: 0.2941\n",
      "Epoch 125/500\n",
      "993/993 [==============================] - 0s 138us/step - loss: 2.5286 - acc: 0.2910\n",
      "Epoch 126/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.5281 - acc: 0.2941\n",
      "Epoch 127/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.5264 - acc: 0.2900\n",
      "Epoch 128/500\n",
      "993/993 [==============================] - 0s 130us/step - loss: 2.5248 - acc: 0.2920\n",
      "Epoch 129/500\n",
      "993/993 [==============================] - 0s 129us/step - loss: 2.5233 - acc: 0.2920 0s - loss: 2.5176 - acc: 0.284\n",
      "Epoch 130/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.5224 - acc: 0.2920\n",
      "Epoch 131/500\n",
      "993/993 [==============================] - 0s 121us/step - loss: 2.5202 - acc: 0.2920\n",
      "Epoch 132/500\n",
      "993/993 [==============================] - 0s 126us/step - loss: 2.5192 - acc: 0.2971\n",
      "Epoch 133/500\n",
      "993/993 [==============================] - 0s 127us/step - loss: 2.5175 - acc: 0.2910\n",
      "Epoch 134/500\n",
      "993/993 [==============================] - 0s 131us/step - loss: 2.5156 - acc: 0.2920\n",
      "Epoch 135/500\n",
      "993/993 [==============================] - 0s 127us/step - loss: 2.5144 - acc: 0.2941\n",
      "Epoch 136/500\n",
      "993/993 [==============================] - 0s 121us/step - loss: 2.5133 - acc: 0.2931\n",
      "Epoch 137/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.5127 - acc: 0.2941\n",
      "Epoch 138/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.5108 - acc: 0.2941\n",
      "Epoch 139/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.5100 - acc: 0.2880\n",
      "Epoch 140/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.5085 - acc: 0.2981\n",
      "Epoch 141/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.5062 - acc: 0.2961\n",
      "Epoch 142/500\n",
      "993/993 [==============================] - 0s 137us/step - loss: 2.5055 - acc: 0.2920\n",
      "Epoch 143/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.5042 - acc: 0.2900\n",
      "Epoch 144/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.5029 - acc: 0.2890\n",
      "Epoch 145/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.5017 - acc: 0.2870\n",
      "Epoch 146/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.4996 - acc: 0.2880\n",
      "Epoch 147/500\n",
      "993/993 [==============================] - 0s 126us/step - loss: 2.4989 - acc: 0.2870\n",
      "Epoch 148/500\n",
      "993/993 [==============================] - 0s 121us/step - loss: 2.4969 - acc: 0.2890\n",
      "Epoch 149/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.4969 - acc: 0.2870\n",
      "Epoch 150/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.4943 - acc: 0.2900\n",
      "Epoch 151/500\n",
      "993/993 [==============================] - 0s 136us/step - loss: 2.4947 - acc: 0.2890\n",
      "Epoch 152/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.4928 - acc: 0.2900\n",
      "Epoch 153/500\n",
      "993/993 [==============================] - 0s 121us/step - loss: 2.4915 - acc: 0.2910\n",
      "Epoch 154/500\n",
      "993/993 [==============================] - 0s 126us/step - loss: 2.4897 - acc: 0.2951\n",
      "Epoch 155/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.4887 - acc: 0.2920\n",
      "Epoch 156/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.4869 - acc: 0.2941\n",
      "Epoch 157/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.4859 - acc: 0.2910\n",
      "Epoch 158/500\n",
      "993/993 [==============================] - 0s 125us/step - loss: 2.4853 - acc: 0.2971\n",
      "Epoch 159/500\n",
      "993/993 [==============================] - 0s 138us/step - loss: 2.4842 - acc: 0.2920\n",
      "Epoch 160/500\n",
      "993/993 [==============================] - 0s 119us/step - loss: 2.4828 - acc: 0.2900\n",
      "Epoch 161/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.4805 - acc: 0.2920\n",
      "Epoch 162/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.4793 - acc: 0.2920\n",
      "Epoch 163/500\n",
      "993/993 [==============================] - 0s 124us/step - loss: 2.4787 - acc: 0.2830\n",
      "Epoch 164/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993/993 [==============================] - 0s 131us/step - loss: 2.4777 - acc: 0.2860\n",
      "Epoch 165/500\n",
      "993/993 [==============================] - 0s 105us/step - loss: 2.4765 - acc: 0.2971\n",
      "Epoch 166/500\n",
      "993/993 [==============================] - 0s 105us/step - loss: 2.4757 - acc: 0.2880\n",
      "Epoch 167/500\n",
      "993/993 [==============================] - 0s 104us/step - loss: 2.4736 - acc: 0.2941\n",
      "Epoch 168/500\n",
      "993/993 [==============================] - 0s 115us/step - loss: 2.4727 - acc: 0.2890\n",
      "Epoch 169/500\n",
      "993/993 [==============================] - 0s 99us/step - loss: 2.4714 - acc: 0.2910\n",
      "Epoch 170/500\n",
      "993/993 [==============================] - 0s 100us/step - loss: 2.4709 - acc: 0.2910\n",
      "Epoch 171/500\n",
      "993/993 [==============================] - 0s 101us/step - loss: 2.4693 - acc: 0.2890\n",
      "Epoch 172/500\n",
      "993/993 [==============================] - 0s 100us/step - loss: 2.4675 - acc: 0.2870\n",
      "Epoch 173/500\n",
      "993/993 [==============================] - 0s 101us/step - loss: 2.4669 - acc: 0.2920\n",
      "Epoch 174/500\n",
      "993/993 [==============================] - 0s 113us/step - loss: 2.4657 - acc: 0.2890\n",
      "Epoch 175/500\n",
      "993/993 [==============================] - 0s 155us/step - loss: 2.4637 - acc: 0.2890\n",
      "Epoch 176/500\n",
      "993/993 [==============================] - 0s 130us/step - loss: 2.4631 - acc: 0.2920\n",
      "Epoch 177/500\n",
      "993/993 [==============================] - 0s 135us/step - loss: 2.4626 - acc: 0.2920\n",
      "Epoch 178/500\n",
      "993/993 [==============================] - 0s 130us/step - loss: 2.4614 - acc: 0.2850\n",
      "Epoch 179/500\n",
      "993/993 [==============================] - 0s 130us/step - loss: 2.4591 - acc: 0.2910\n",
      "Epoch 180/500\n",
      "993/993 [==============================] - 0s 112us/step - loss: 2.4582 - acc: 0.2961\n",
      "Epoch 181/500\n",
      "993/993 [==============================] - 0s 104us/step - loss: 2.4586 - acc: 0.2951\n",
      "Epoch 182/500\n",
      "993/993 [==============================] - 0s 102us/step - loss: 2.4560 - acc: 0.2920\n",
      "Epoch 183/500\n",
      "993/993 [==============================] - 0s 100us/step - loss: 2.4551 - acc: 0.2971\n",
      "Epoch 184/500\n",
      "993/993 [==============================] - 0s 104us/step - loss: 2.4539 - acc: 0.2880\n",
      "Epoch 185/500\n",
      "993/993 [==============================] - 0s 100us/step - loss: 2.4533 - acc: 0.2951\n",
      "Epoch 186/500\n",
      "993/993 [==============================] - 0s 99us/step - loss: 2.4517 - acc: 0.2931\n",
      "Epoch 187/500\n",
      "993/993 [==============================] - 0s 118us/step - loss: 2.4510 - acc: 0.2941\n",
      "Epoch 188/500\n",
      "993/993 [==============================] - 0s 101us/step - loss: 2.4503 - acc: 0.3001\n",
      "Epoch 189/500\n",
      "993/993 [==============================] - 0s 103us/step - loss: 2.4485 - acc: 0.2981\n",
      "Epoch 190/500\n",
      "993/993 [==============================] - 0s 100us/step - loss: 2.4465 - acc: 0.2961\n",
      "Epoch 191/500\n",
      "993/993 [==============================] - 0s 103us/step - loss: 2.4466 - acc: 0.2931\n",
      "Epoch 192/500\n",
      "993/993 [==============================] - 0s 118us/step - loss: 2.4455 - acc: 0.2900\n",
      "Epoch 193/500\n",
      "993/993 [==============================] - 0s 105us/step - loss: 2.4447 - acc: 0.2941\n",
      "Epoch 194/500\n",
      "993/993 [==============================] - 0s 101us/step - loss: 2.4424 - acc: 0.2890\n",
      "Epoch 195/500\n",
      "993/993 [==============================] - 0s 100us/step - loss: 2.4419 - acc: 0.2900\n",
      "Epoch 196/500\n",
      "993/993 [==============================] - 0s 100us/step - loss: 2.4406 - acc: 0.2931\n",
      "Epoch 197/500\n",
      "993/993 [==============================] - 0s 128us/step - loss: 2.4404 - acc: 0.2951\n",
      "Epoch 198/500\n",
      "993/993 [==============================] - 0s 103us/step - loss: 2.4388 - acc: 0.2961\n",
      "Epoch 199/500\n",
      "993/993 [==============================] - 0s 100us/step - loss: 2.4387 - acc: 0.2991\n",
      "Epoch 200/500\n",
      "993/993 [==============================] - 0s 100us/step - loss: 2.4378 - acc: 0.2910\n",
      "Epoch 201/500\n",
      "993/993 [==============================] - 0s 100us/step - loss: 2.4365 - acc: 0.2951\n",
      "Epoch 202/500\n",
      "993/993 [==============================] - 0s 114us/step - loss: 2.4344 - acc: 0.2991\n",
      "Epoch 203/500\n",
      "993/993 [==============================] - 0s 100us/step - loss: 2.4344 - acc: 0.2971\n",
      "Epoch 204/500\n",
      "993/993 [==============================] - 0s 132us/step - loss: 2.4324 - acc: 0.2920\n",
      "Epoch 205/500\n",
      "993/993 [==============================] - 0s 136us/step - loss: 2.4320 - acc: 0.2961\n",
      "Epoch 206/500\n",
      "993/993 [==============================] - 0s 121us/step - loss: 2.4308 - acc: 0.2920\n",
      "Epoch 207/500\n",
      "993/993 [==============================] - 0s 111us/step - loss: 2.4294 - acc: 0.2910\n",
      "Epoch 208/500\n",
      "993/993 [==============================] - 0s 107us/step - loss: 2.4283 - acc: 0.2900\n",
      "Epoch 209/500\n",
      "993/993 [==============================] - 0s 102us/step - loss: 2.4276 - acc: 0.2981\n",
      "Epoch 210/500\n",
      "993/993 [==============================] - 0s 117us/step - loss: 2.4268 - acc: 0.3011\n",
      "Epoch 211/500\n",
      "993/993 [==============================] - 0s 101us/step - loss: 2.4255 - acc: 0.2991\n",
      "Epoch 212/500\n",
      "993/993 [==============================] - 0s 100us/step - loss: 2.4247 - acc: 0.2971\n",
      "Epoch 213/500\n",
      "993/993 [==============================] - 0s 100us/step - loss: 2.4232 - acc: 0.2951\n",
      "Epoch 214/500\n",
      "993/993 [==============================] - 0s 114us/step - loss: 2.4231 - acc: 0.3001\n",
      "Epoch 215/500\n",
      "993/993 [==============================] - 0s 107us/step - loss: 2.4216 - acc: 0.2981\n",
      "Epoch 216/500\n",
      "993/993 [==============================] - 0s 113us/step - loss: 2.4200 - acc: 0.2991\n",
      "Epoch 217/500\n",
      "993/993 [==============================] - 0s 104us/step - loss: 2.4186 - acc: 0.2951\n",
      "Epoch 218/500\n",
      "993/993 [==============================] - 0s 98us/step - loss: 2.4187 - acc: 0.3001\n",
      "Epoch 219/500\n",
      "993/993 [==============================] - 0s 101us/step - loss: 2.4176 - acc: 0.3021\n",
      "Epoch 220/500\n",
      "993/993 [==============================] - 0s 119us/step - loss: 2.4151 - acc: 0.3001\n",
      "Epoch 221/500\n",
      "993/993 [==============================] - 0s 103us/step - loss: 2.4153 - acc: 0.2991\n",
      "Epoch 222/500\n",
      "993/993 [==============================] - 0s 102us/step - loss: 2.4137 - acc: 0.3051\n",
      "Epoch 223/500\n",
      "993/993 [==============================] - 0s 104us/step - loss: 2.4128 - acc: 0.2981\n",
      "Epoch 224/500\n",
      "993/993 [==============================] - 0s 113us/step - loss: 2.4134 - acc: 0.2961\n",
      "Epoch 225/500\n",
      "993/993 [==============================] - 0s 103us/step - loss: 2.4115 - acc: 0.2971\n",
      "Epoch 226/500\n",
      "993/993 [==============================] - 0s 118us/step - loss: 2.4110 - acc: 0.2971\n",
      "Epoch 227/500\n",
      "993/993 [==============================] - 0s 109us/step - loss: 2.4103 - acc: 0.2931\n",
      "Epoch 228/500\n",
      "993/993 [==============================] - 0s 106us/step - loss: 2.4082 - acc: 0.3021\n",
      "Epoch 229/500\n",
      "993/993 [==============================] - 0s 105us/step - loss: 2.4089 - acc: 0.3001\n",
      "Epoch 230/500\n",
      "993/993 [==============================] - 0s 98us/step - loss: 2.4064 - acc: 0.3011\n",
      "Epoch 231/500\n",
      "993/993 [==============================] - 0s 112us/step - loss: 2.4065 - acc: 0.3011\n",
      "Epoch 232/500\n",
      "993/993 [==============================] - 0s 100us/step - loss: 2.4051 - acc: 0.3041\n",
      "Epoch 233/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.4037 - acc: 0.2991\n",
      "Epoch 234/500\n",
      "993/993 [==============================] - 0s 144us/step - loss: 2.4044 - acc: 0.2961\n",
      "Epoch 235/500\n",
      "993/993 [==============================] - 0s 170us/step - loss: 2.4026 - acc: 0.3041\n",
      "Epoch 236/500\n",
      "993/993 [==============================] - 0s 157us/step - loss: 2.4022 - acc: 0.3031\n",
      "Epoch 237/500\n",
      "993/993 [==============================] - 0s 195us/step - loss: 2.4007 - acc: 0.3031\n",
      "Epoch 238/500\n",
      "993/993 [==============================] - 0s 188us/step - loss: 2.3994 - acc: 0.3082\n",
      "Epoch 239/500\n",
      "993/993 [==============================] - 0s 156us/step - loss: 2.3984 - acc: 0.3001\n",
      "Epoch 240/500\n",
      "993/993 [==============================] - 0s 171us/step - loss: 2.3968 - acc: 0.3031\n",
      "Epoch 241/500\n",
      "993/993 [==============================] - 0s 187us/step - loss: 2.3963 - acc: 0.3041\n",
      "Epoch 242/500\n",
      "993/993 [==============================] - 0s 244us/step - loss: 2.3959 - acc: 0.3011\n",
      "Epoch 243/500\n",
      "993/993 [==============================] - 0s 212us/step - loss: 2.3954 - acc: 0.3051\n",
      "Epoch 244/500\n",
      "993/993 [==============================] - 0s 227us/step - loss: 2.3936 - acc: 0.3092\n",
      "Epoch 245/500\n",
      "993/993 [==============================] - 0s 225us/step - loss: 2.3928 - acc: 0.3011\n",
      "Epoch 246/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993/993 [==============================] - 0s 234us/step - loss: 2.3925 - acc: 0.2981\n",
      "Epoch 247/500\n",
      "993/993 [==============================] - 0s 208us/step - loss: 2.3911 - acc: 0.3011\n",
      "Epoch 248/500\n",
      "993/993 [==============================] - ETA: 0s - loss: 2.3530 - acc: 0.311 - 0s 251us/step - loss: 2.3902 - acc: 0.3001\n",
      "Epoch 249/500\n",
      "993/993 [==============================] - 0s 228us/step - loss: 2.3887 - acc: 0.2951\n",
      "Epoch 250/500\n",
      "993/993 [==============================] - 0s 176us/step - loss: 2.3888 - acc: 0.2961\n",
      "Epoch 251/500\n",
      "993/993 [==============================] - 0s 190us/step - loss: 2.3878 - acc: 0.3021\n",
      "Epoch 252/500\n",
      "993/993 [==============================] - 0s 238us/step - loss: 2.3872 - acc: 0.2961\n",
      "Epoch 253/500\n",
      "993/993 [==============================] - 0s 268us/step - loss: 2.3858 - acc: 0.3082\n",
      "Epoch 254/500\n",
      "993/993 [==============================] - 0s 207us/step - loss: 2.3846 - acc: 0.3001\n",
      "Epoch 255/500\n",
      "993/993 [==============================] - 0s 179us/step - loss: 2.3840 - acc: 0.2991\n",
      "Epoch 256/500\n",
      "993/993 [==============================] - 0s 187us/step - loss: 2.3828 - acc: 0.3011\n",
      "Epoch 257/500\n",
      "993/993 [==============================] - 0s 204us/step - loss: 2.3827 - acc: 0.2961\n",
      "Epoch 258/500\n",
      "993/993 [==============================] - 0s 198us/step - loss: 2.3816 - acc: 0.3001\n",
      "Epoch 259/500\n",
      "993/993 [==============================] - 0s 195us/step - loss: 2.3802 - acc: 0.2971\n",
      "Epoch 260/500\n",
      "993/993 [==============================] - 0s 216us/step - loss: 2.3800 - acc: 0.2981\n",
      "Epoch 261/500\n",
      "993/993 [==============================] - 0s 180us/step - loss: 2.3797 - acc: 0.3041\n",
      "Epoch 262/500\n",
      "993/993 [==============================] - 0s 180us/step - loss: 2.3772 - acc: 0.3011\n",
      "Epoch 263/500\n",
      "993/993 [==============================] - 0s 177us/step - loss: 2.3775 - acc: 0.3082\n",
      "Epoch 264/500\n",
      "993/993 [==============================] - 0s 190us/step - loss: 2.3775 - acc: 0.2971\n",
      "Epoch 265/500\n",
      "993/993 [==============================] - 0s 162us/step - loss: 2.3768 - acc: 0.3001\n",
      "Epoch 266/500\n",
      "993/993 [==============================] - 0s 151us/step - loss: 2.3740 - acc: 0.3001\n",
      "Epoch 267/500\n",
      "993/993 [==============================] - 0s 156us/step - loss: 2.3741 - acc: 0.3011\n",
      "Epoch 268/500\n",
      "993/993 [==============================] - 0s 144us/step - loss: 2.3729 - acc: 0.3001\n",
      "Epoch 269/500\n",
      "993/993 [==============================] - 0s 132us/step - loss: 2.3724 - acc: 0.3021\n",
      "Epoch 270/500\n",
      "993/993 [==============================] - 0s 160us/step - loss: 2.3710 - acc: 0.3011\n",
      "Epoch 271/500\n",
      "993/993 [==============================] - 0s 128us/step - loss: 2.3704 - acc: 0.2971\n",
      "Epoch 272/500\n",
      "993/993 [==============================] - 0s 130us/step - loss: 2.3696 - acc: 0.3001\n",
      "Epoch 273/500\n",
      "993/993 [==============================] - 0s 128us/step - loss: 2.3680 - acc: 0.3041\n",
      "Epoch 274/500\n",
      "993/993 [==============================] - 0s 129us/step - loss: 2.3679 - acc: 0.2971\n",
      "Epoch 275/500\n",
      "993/993 [==============================] - 0s 177us/step - loss: 2.3667 - acc: 0.3051\n",
      "Epoch 276/500\n",
      "993/993 [==============================] - 0s 191us/step - loss: 2.3665 - acc: 0.2941\n",
      "Epoch 277/500\n",
      "993/993 [==============================] - 0s 180us/step - loss: 2.3660 - acc: 0.2981\n",
      "Epoch 278/500\n",
      "993/993 [==============================] - 0s 163us/step - loss: 2.3653 - acc: 0.3011\n",
      "Epoch 279/500\n",
      "993/993 [==============================] - 0s 133us/step - loss: 2.3639 - acc: 0.3001\n",
      "Epoch 280/500\n",
      "993/993 [==============================] - ETA: 0s - loss: 2.3283 - acc: 0.309 - 0s 154us/step - loss: 2.3630 - acc: 0.2991\n",
      "Epoch 281/500\n",
      "993/993 [==============================] - 0s 143us/step - loss: 2.3614 - acc: 0.2991\n",
      "Epoch 282/500\n",
      "993/993 [==============================] - 0s 174us/step - loss: 2.3612 - acc: 0.3041\n",
      "Epoch 283/500\n",
      "993/993 [==============================] - 0s 150us/step - loss: 2.3607 - acc: 0.2981\n",
      "Epoch 284/500\n",
      "993/993 [==============================] - 0s 155us/step - loss: 2.3604 - acc: 0.2951\n",
      "Epoch 285/500\n",
      "993/993 [==============================] - 0s 129us/step - loss: 2.3594 - acc: 0.2981\n",
      "Epoch 286/500\n",
      "993/993 [==============================] - 0s 160us/step - loss: 2.3587 - acc: 0.2971\n",
      "Epoch 287/500\n",
      "993/993 [==============================] - 0s 135us/step - loss: 2.3587 - acc: 0.2961\n",
      "Epoch 288/500\n",
      "993/993 [==============================] - 0s 135us/step - loss: 2.3574 - acc: 0.2991\n",
      "Epoch 289/500\n",
      "993/993 [==============================] - 0s 142us/step - loss: 2.3565 - acc: 0.3051\n",
      "Epoch 290/500\n",
      "993/993 [==============================] - 0s 151us/step - loss: 2.3551 - acc: 0.3001\n",
      "Epoch 291/500\n",
      "993/993 [==============================] - 0s 143us/step - loss: 2.3537 - acc: 0.3001\n",
      "Epoch 292/500\n",
      "993/993 [==============================] - 0s 144us/step - loss: 2.3538 - acc: 0.3021\n",
      "Epoch 293/500\n",
      "993/993 [==============================] - 0s 138us/step - loss: 2.3530 - acc: 0.3011\n",
      "Epoch 294/500\n",
      "993/993 [==============================] - 0s 147us/step - loss: 2.3519 - acc: 0.3041\n",
      "Epoch 295/500\n",
      "993/993 [==============================] - 0s 142us/step - loss: 2.3513 - acc: 0.2981\n",
      "Epoch 296/500\n",
      "993/993 [==============================] - 0s 154us/step - loss: 2.3499 - acc: 0.3021\n",
      "Epoch 297/500\n",
      "993/993 [==============================] - 0s 143us/step - loss: 2.3492 - acc: 0.3041\n",
      "Epoch 298/500\n",
      "993/993 [==============================] - 0s 142us/step - loss: 2.3491 - acc: 0.2981\n",
      "Epoch 299/500\n",
      "993/993 [==============================] - 0s 162us/step - loss: 2.3483 - acc: 0.2991\n",
      "Epoch 300/500\n",
      "993/993 [==============================] - 0s 128us/step - loss: 2.3471 - acc: 0.2971\n",
      "Epoch 301/500\n",
      "993/993 [==============================] - 0s 153us/step - loss: 2.3467 - acc: 0.3021\n",
      "Epoch 302/500\n",
      "993/993 [==============================] - 0s 136us/step - loss: 2.3459 - acc: 0.2971\n",
      "Epoch 303/500\n",
      "993/993 [==============================] - 0s 150us/step - loss: 2.3457 - acc: 0.2971\n",
      "Epoch 304/500\n",
      "993/993 [==============================] - 0s 145us/step - loss: 2.3438 - acc: 0.3021\n",
      "Epoch 305/500\n",
      "993/993 [==============================] - 0s 149us/step - loss: 2.3435 - acc: 0.3001\n",
      "Epoch 306/500\n",
      "993/993 [==============================] - 0s 158us/step - loss: 2.3426 - acc: 0.3011\n",
      "Epoch 307/500\n",
      "993/993 [==============================] - 0s 144us/step - loss: 2.3419 - acc: 0.3021\n",
      "Epoch 308/500\n",
      "993/993 [==============================] - 0s 153us/step - loss: 2.3420 - acc: 0.3021\n",
      "Epoch 309/500\n",
      "993/993 [==============================] - 0s 144us/step - loss: 2.3395 - acc: 0.3021\n",
      "Epoch 310/500\n",
      "993/993 [==============================] - 0s 151us/step - loss: 2.3401 - acc: 0.2951\n",
      "Epoch 311/500\n",
      "993/993 [==============================] - 0s 140us/step - loss: 2.3411 - acc: 0.2961\n",
      "Epoch 312/500\n",
      "993/993 [==============================] - 0s 140us/step - loss: 2.3386 - acc: 0.2951\n",
      "Epoch 313/500\n",
      "993/993 [==============================] - 0s 188us/step - loss: 2.3370 - acc: 0.3011\n",
      "Epoch 314/500\n",
      "993/993 [==============================] - 0s 195us/step - loss: 2.3363 - acc: 0.3011\n",
      "Epoch 315/500\n",
      "993/993 [==============================] - 0s 174us/step - loss: 2.3355 - acc: 0.3051\n",
      "Epoch 316/500\n",
      "993/993 [==============================] - 0s 172us/step - loss: 2.3355 - acc: 0.3041\n",
      "Epoch 317/500\n",
      "993/993 [==============================] - 0s 181us/step - loss: 2.3350 - acc: 0.3011\n",
      "Epoch 318/500\n",
      "993/993 [==============================] - 0s 160us/step - loss: 2.3351 - acc: 0.2910\n",
      "Epoch 319/500\n",
      "993/993 [==============================] - 0s 164us/step - loss: 2.3336 - acc: 0.3021\n",
      "Epoch 320/500\n",
      "993/993 [==============================] - 0s 165us/step - loss: 2.3328 - acc: 0.3021\n",
      "Epoch 321/500\n",
      "993/993 [==============================] - 0s 154us/step - loss: 2.3333 - acc: 0.2981\n",
      "Epoch 322/500\n",
      "993/993 [==============================] - 0s 142us/step - loss: 2.3313 - acc: 0.2951\n",
      "Epoch 323/500\n",
      "993/993 [==============================] - 0s 154us/step - loss: 2.3305 - acc: 0.3031\n",
      "Epoch 324/500\n",
      "993/993 [==============================] - 0s 152us/step - loss: 2.3306 - acc: 0.2971\n",
      "Epoch 325/500\n",
      "993/993 [==============================] - 0s 154us/step - loss: 2.3299 - acc: 0.3011\n",
      "Epoch 326/500\n",
      "993/993 [==============================] - 0s 148us/step - loss: 2.3303 - acc: 0.3061\n",
      "Epoch 327/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993/993 [==============================] - 0s 171us/step - loss: 2.3279 - acc: 0.3021\n",
      "Epoch 328/500\n",
      "993/993 [==============================] - 0s 125us/step - loss: 2.3273 - acc: 0.3041\n",
      "Epoch 329/500\n",
      "993/993 [==============================] - 0s 136us/step - loss: 2.3267 - acc: 0.3051\n",
      "Epoch 330/500\n",
      "993/993 [==============================] - 0s 132us/step - loss: 2.3259 - acc: 0.2971\n",
      "Epoch 331/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.3255 - acc: 0.2981\n",
      "Epoch 332/500\n",
      "993/993 [==============================] - 0s 129us/step - loss: 2.3241 - acc: 0.3041\n",
      "Epoch 333/500\n",
      "993/993 [==============================] - 0s 126us/step - loss: 2.3237 - acc: 0.2961\n",
      "Epoch 334/500\n",
      "993/993 [==============================] - 0s 170us/step - loss: 2.3230 - acc: 0.2981\n",
      "Epoch 335/500\n",
      "993/993 [==============================] - 0s 150us/step - loss: 2.3221 - acc: 0.2961\n",
      "Epoch 336/500\n",
      "993/993 [==============================] - 0s 133us/step - loss: 2.3212 - acc: 0.3021\n",
      "Epoch 337/500\n",
      "993/993 [==============================] - 0s 132us/step - loss: 2.3209 - acc: 0.3011\n",
      "Epoch 338/500\n",
      "993/993 [==============================] - 0s 134us/step - loss: 2.3206 - acc: 0.3011\n",
      "Epoch 339/500\n",
      "993/993 [==============================] - 0s 138us/step - loss: 2.3201 - acc: 0.2981\n",
      "Epoch 340/500\n",
      "993/993 [==============================] - 0s 127us/step - loss: 2.3190 - acc: 0.2991\n",
      "Epoch 341/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.3170 - acc: 0.3041\n",
      "Epoch 342/500\n",
      "993/993 [==============================] - 0s 155us/step - loss: 2.3178 - acc: 0.3001\n",
      "Epoch 343/500\n",
      "993/993 [==============================] - 0s 140us/step - loss: 2.3171 - acc: 0.2971\n",
      "Epoch 344/500\n",
      "993/993 [==============================] - 0s 129us/step - loss: 2.3163 - acc: 0.2991\n",
      "Epoch 345/500\n",
      "993/993 [==============================] - 0s 121us/step - loss: 2.3157 - acc: 0.3021\n",
      "Epoch 346/500\n",
      "993/993 [==============================] - 0s 134us/step - loss: 2.3147 - acc: 0.2981\n",
      "Epoch 347/500\n",
      "993/993 [==============================] - ETA: 0s - loss: 2.3042 - acc: 0.301 - 0s 154us/step - loss: 2.3144 - acc: 0.3001\n",
      "Epoch 348/500\n",
      "993/993 [==============================] - 0s 127us/step - loss: 2.3139 - acc: 0.2971\n",
      "Epoch 349/500\n",
      "993/993 [==============================] - 0s 134us/step - loss: 2.3131 - acc: 0.3031\n",
      "Epoch 350/500\n",
      "993/993 [==============================] - 0s 143us/step - loss: 2.3116 - acc: 0.3021\n",
      "Epoch 351/500\n",
      "993/993 [==============================] - 0s 143us/step - loss: 2.3114 - acc: 0.3001\n",
      "Epoch 352/500\n",
      "993/993 [==============================] - 0s 139us/step - loss: 2.3108 - acc: 0.2931\n",
      "Epoch 353/500\n",
      "993/993 [==============================] - 0s 151us/step - loss: 2.3102 - acc: 0.2991\n",
      "Epoch 354/500\n",
      "993/993 [==============================] - 0s 136us/step - loss: 2.3094 - acc: 0.2991\n",
      "Epoch 355/500\n",
      "993/993 [==============================] - 0s 141us/step - loss: 2.3080 - acc: 0.2991\n",
      "Epoch 356/500\n",
      "993/993 [==============================] - 0s 140us/step - loss: 2.3087 - acc: 0.3011\n",
      "Epoch 357/500\n",
      "993/993 [==============================] - 0s 143us/step - loss: 2.3075 - acc: 0.3021\n",
      "Epoch 358/500\n",
      "993/993 [==============================] - 0s 158us/step - loss: 2.3072 - acc: 0.3031\n",
      "Epoch 359/500\n",
      "993/993 [==============================] - 0s 130us/step - loss: 2.3071 - acc: 0.2971\n",
      "Epoch 360/500\n",
      "993/993 [==============================] - 0s 144us/step - loss: 2.3055 - acc: 0.3011\n",
      "Epoch 361/500\n",
      "993/993 [==============================] - 0s 140us/step - loss: 2.3051 - acc: 0.3011\n",
      "Epoch 362/500\n",
      "993/993 [==============================] - 0s 139us/step - loss: 2.3038 - acc: 0.2910\n",
      "Epoch 363/500\n",
      "993/993 [==============================] - 0s 141us/step - loss: 2.3048 - acc: 0.2941\n",
      "Epoch 364/500\n",
      "993/993 [==============================] - 0s 136us/step - loss: 2.3037 - acc: 0.2991\n",
      "Epoch 365/500\n",
      "993/993 [==============================] - 0s 144us/step - loss: 2.3025 - acc: 0.3041\n",
      "Epoch 366/500\n",
      "993/993 [==============================] - 0s 144us/step - loss: 2.3015 - acc: 0.2941\n",
      "Epoch 367/500\n",
      "993/993 [==============================] - 0s 138us/step - loss: 2.3023 - acc: 0.2981\n",
      "Epoch 368/500\n",
      "993/993 [==============================] - 0s 135us/step - loss: 2.3013 - acc: 0.3021\n",
      "Epoch 369/500\n",
      "993/993 [==============================] - 0s 139us/step - loss: 2.3004 - acc: 0.2971\n",
      "Epoch 370/500\n",
      "993/993 [==============================] - 0s 142us/step - loss: 2.2999 - acc: 0.2981\n",
      "Epoch 371/500\n",
      "993/993 [==============================] - 0s 141us/step - loss: 2.2988 - acc: 0.2941\n",
      "Epoch 372/500\n",
      "993/993 [==============================] - 0s 144us/step - loss: 2.2982 - acc: 0.3021\n",
      "Epoch 373/500\n",
      "993/993 [==============================] - 0s 180us/step - loss: 2.2978 - acc: 0.2981\n",
      "Epoch 374/500\n",
      "993/993 [==============================] - 0s 160us/step - loss: 2.2968 - acc: 0.2951\n",
      "Epoch 375/500\n",
      "993/993 [==============================] - 0s 138us/step - loss: 2.2957 - acc: 0.3011\n",
      "Epoch 376/500\n",
      "993/993 [==============================] - 0s 154us/step - loss: 2.2956 - acc: 0.2991\n",
      "Epoch 377/500\n",
      "993/993 [==============================] - 0s 144us/step - loss: 2.2957 - acc: 0.2941\n",
      "Epoch 378/500\n",
      "993/993 [==============================] - 0s 172us/step - loss: 2.2946 - acc: 0.2941\n",
      "Epoch 379/500\n",
      "993/993 [==============================] - 0s 148us/step - loss: 2.2939 - acc: 0.2981\n",
      "Epoch 380/500\n",
      "993/993 [==============================] - 0s 153us/step - loss: 2.2934 - acc: 0.2961\n",
      "Epoch 381/500\n",
      "993/993 [==============================] - 0s 158us/step - loss: 2.2931 - acc: 0.2991\n",
      "Epoch 382/500\n",
      "993/993 [==============================] - 0s 176us/step - loss: 2.2916 - acc: 0.3051\n",
      "Epoch 383/500\n",
      "993/993 [==============================] - 0s 168us/step - loss: 2.2916 - acc: 0.3011\n",
      "Epoch 384/500\n",
      "993/993 [==============================] - 0s 173us/step - loss: 2.2899 - acc: 0.2961\n",
      "Epoch 385/500\n",
      "993/993 [==============================] - 0s 171us/step - loss: 2.2906 - acc: 0.3051\n",
      "Epoch 386/500\n",
      "993/993 [==============================] - 0s 174us/step - loss: 2.2890 - acc: 0.2961\n",
      "Epoch 387/500\n",
      "993/993 [==============================] - 0s 185us/step - loss: 2.2895 - acc: 0.2890\n",
      "Epoch 388/500\n",
      "993/993 [==============================] - 0s 162us/step - loss: 2.2884 - acc: 0.2951\n",
      "Epoch 389/500\n",
      "993/993 [==============================] - 0s 130us/step - loss: 2.2873 - acc: 0.2971\n",
      "Epoch 390/500\n",
      "993/993 [==============================] - 0s 150us/step - loss: 2.2876 - acc: 0.2971\n",
      "Epoch 391/500\n",
      "993/993 [==============================] - 0s 150us/step - loss: 2.2875 - acc: 0.2931\n",
      "Epoch 392/500\n",
      "993/993 [==============================] - 0s 147us/step - loss: 2.2866 - acc: 0.2971\n",
      "Epoch 393/500\n",
      "993/993 [==============================] - 0s 157us/step - loss: 2.2861 - acc: 0.2971\n",
      "Epoch 394/500\n",
      "993/993 [==============================] - 0s 136us/step - loss: 2.2848 - acc: 0.2991\n",
      "Epoch 395/500\n",
      "993/993 [==============================] - 0s 163us/step - loss: 2.2843 - acc: 0.3001\n",
      "Epoch 396/500\n",
      "993/993 [==============================] - 0s 165us/step - loss: 2.2843 - acc: 0.2910\n",
      "Epoch 397/500\n",
      "993/993 [==============================] - 0s 150us/step - loss: 2.2826 - acc: 0.2961\n",
      "Epoch 398/500\n",
      "993/993 [==============================] - 0s 141us/step - loss: 2.2826 - acc: 0.2971\n",
      "Epoch 399/500\n",
      "993/993 [==============================] - 0s 160us/step - loss: 2.2827 - acc: 0.2971\n",
      "Epoch 400/500\n",
      "993/993 [==============================] - 0s 153us/step - loss: 2.2821 - acc: 0.2961\n",
      "Epoch 401/500\n",
      "993/993 [==============================] - 0s 147us/step - loss: 2.2814 - acc: 0.2910\n",
      "Epoch 402/500\n",
      "993/993 [==============================] - 0s 143us/step - loss: 2.2804 - acc: 0.2971\n",
      "Epoch 403/500\n",
      "993/993 [==============================] - 0s 139us/step - loss: 2.2795 - acc: 0.3021\n",
      "Epoch 404/500\n",
      "993/993 [==============================] - 0s 149us/step - loss: 2.2797 - acc: 0.3001\n",
      "Epoch 405/500\n",
      "993/993 [==============================] - 0s 139us/step - loss: 2.2778 - acc: 0.2981\n",
      "Epoch 406/500\n",
      "993/993 [==============================] - 0s 151us/step - loss: 2.2794 - acc: 0.2961\n",
      "Epoch 407/500\n",
      "993/993 [==============================] - 0s 158us/step - loss: 2.2767 - acc: 0.3011\n",
      "Epoch 408/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993/993 [==============================] - 0s 147us/step - loss: 2.2766 - acc: 0.2880\n",
      "Epoch 409/500\n",
      "993/993 [==============================] - 0s 132us/step - loss: 2.2763 - acc: 0.3001\n",
      "Epoch 410/500\n",
      "993/993 [==============================] - 0s 119us/step - loss: 2.2752 - acc: 0.3011\n",
      "Epoch 411/500\n",
      "993/993 [==============================] - 0s 132us/step - loss: 2.2749 - acc: 0.2951\n",
      "Epoch 412/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.2747 - acc: 0.3001\n",
      "Epoch 413/500\n",
      "993/993 [==============================] - 0s 113us/step - loss: 2.2736 - acc: 0.3001\n",
      "Epoch 414/500\n",
      "993/993 [==============================] - 0s 128us/step - loss: 2.2732 - acc: 0.3011\n",
      "Epoch 415/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.2727 - acc: 0.2941\n",
      "Epoch 416/500\n",
      "993/993 [==============================] - 0s 138us/step - loss: 2.2724 - acc: 0.2931\n",
      "Epoch 417/500\n",
      "993/993 [==============================] - 0s 122us/step - loss: 2.2719 - acc: 0.2890\n",
      "Epoch 418/500\n",
      "993/993 [==============================] - 0s 125us/step - loss: 2.2720 - acc: 0.2971\n",
      "Epoch 419/500\n",
      "993/993 [==============================] - 0s 120us/step - loss: 2.2704 - acc: 0.2931\n",
      "Epoch 420/500\n",
      "993/993 [==============================] - 0s 117us/step - loss: 2.2701 - acc: 0.2951\n",
      "Epoch 421/500\n",
      "993/993 [==============================] - 0s 138us/step - loss: 2.2691 - acc: 0.2971\n",
      "Epoch 422/500\n",
      "993/993 [==============================] - 0s 137us/step - loss: 2.2691 - acc: 0.2920\n",
      "Epoch 423/500\n",
      "993/993 [==============================] - 0s 148us/step - loss: 2.2677 - acc: 0.2961\n",
      "Epoch 424/500\n",
      "993/993 [==============================] - 0s 147us/step - loss: 2.2677 - acc: 0.2931\n",
      "Epoch 425/500\n",
      "993/993 [==============================] - 0s 134us/step - loss: 2.2674 - acc: 0.3041\n",
      "Epoch 426/500\n",
      "993/993 [==============================] - 0s 147us/step - loss: 2.2673 - acc: 0.2951\n",
      "Epoch 427/500\n",
      "993/993 [==============================] - 0s 141us/step - loss: 2.2659 - acc: 0.2991\n",
      "Epoch 428/500\n",
      "993/993 [==============================] - 0s 123us/step - loss: 2.2662 - acc: 0.2971\n",
      "Epoch 429/500\n",
      "993/993 [==============================] - 0s 138us/step - loss: 2.2665 - acc: 0.2880\n",
      "Epoch 430/500\n",
      "993/993 [==============================] - 0s 135us/step - loss: 2.2647 - acc: 0.2941\n",
      "Epoch 431/500\n",
      "993/993 [==============================] - 0s 154us/step - loss: 2.2632 - acc: 0.3031\n",
      "Epoch 432/500\n",
      "993/993 [==============================] - 0s 151us/step - loss: 2.2632 - acc: 0.2941\n",
      "Epoch 433/500\n",
      "993/993 [==============================] - 0s 136us/step - loss: 2.2635 - acc: 0.2941\n",
      "Epoch 434/500\n",
      "993/993 [==============================] - 0s 134us/step - loss: 2.2628 - acc: 0.2971\n",
      "Epoch 435/500\n",
      "993/993 [==============================] - 0s 131us/step - loss: 2.2619 - acc: 0.2951\n",
      "Epoch 436/500\n",
      "993/993 [==============================] - 0s 160us/step - loss: 2.2614 - acc: 0.2991\n",
      "Epoch 437/500\n",
      "993/993 [==============================] - 0s 139us/step - loss: 2.2608 - acc: 0.2981\n",
      "Epoch 438/500\n",
      "993/993 [==============================] - 0s 141us/step - loss: 2.2599 - acc: 0.2991\n",
      "Epoch 439/500\n",
      "993/993 [==============================] - 0s 162us/step - loss: 2.2602 - acc: 0.3001\n",
      "Epoch 440/500\n",
      "993/993 [==============================] - 0s 143us/step - loss: 2.2591 - acc: 0.2981\n",
      "Epoch 441/500\n",
      "993/993 [==============================] - 0s 119us/step - loss: 2.2586 - acc: 0.2991\n",
      "Epoch 442/500\n",
      "993/993 [==============================] - 0s 128us/step - loss: 2.2586 - acc: 0.3011\n",
      "Epoch 443/500\n",
      "993/993 [==============================] - 0s 135us/step - loss: 2.2572 - acc: 0.2951\n",
      "Epoch 444/500\n",
      "993/993 [==============================] - 0s 151us/step - loss: 2.2574 - acc: 0.2951\n",
      "Epoch 445/500\n",
      "993/993 [==============================] - 0s 139us/step - loss: 2.2569 - acc: 0.2981\n",
      "Epoch 446/500\n",
      "993/993 [==============================] - 0s 161us/step - loss: 2.2562 - acc: 0.2991\n",
      "Epoch 447/500\n",
      "993/993 [==============================] - 0s 155us/step - loss: 2.2550 - acc: 0.2951\n",
      "Epoch 448/500\n",
      "993/993 [==============================] - 0s 169us/step - loss: 2.2549 - acc: 0.3031\n",
      "Epoch 449/500\n",
      "993/993 [==============================] - 0s 133us/step - loss: 2.2541 - acc: 0.2971\n",
      "Epoch 450/500\n",
      "993/993 [==============================] - 0s 170us/step - loss: 2.2549 - acc: 0.3021\n",
      "Epoch 451/500\n",
      "993/993 [==============================] - 0s 187us/step - loss: 2.2531 - acc: 0.3021\n",
      "Epoch 452/500\n",
      "993/993 [==============================] - 0s 151us/step - loss: 2.2538 - acc: 0.2961\n",
      "Epoch 453/500\n",
      "993/993 [==============================] - 0s 164us/step - loss: 2.2524 - acc: 0.3021\n",
      "Epoch 454/500\n",
      "993/993 [==============================] - 0s 186us/step - loss: 2.2523 - acc: 0.3021\n",
      "Epoch 455/500\n",
      "993/993 [==============================] - 0s 204us/step - loss: 2.2516 - acc: 0.3021\n",
      "Epoch 456/500\n",
      "993/993 [==============================] - 0s 205us/step - loss: 2.2507 - acc: 0.3001\n",
      "Epoch 457/500\n",
      "993/993 [==============================] - 0s 200us/step - loss: 2.2502 - acc: 0.3021\n",
      "Epoch 458/500\n",
      "993/993 [==============================] - 0s 170us/step - loss: 2.2498 - acc: 0.3041\n",
      "Epoch 459/500\n",
      "993/993 [==============================] - 0s 185us/step - loss: 2.2494 - acc: 0.2941 0s - loss: 2.2586 - acc: 0.296\n",
      "Epoch 460/500\n",
      "993/993 [==============================] - 0s 182us/step - loss: 2.2489 - acc: 0.2991\n",
      "Epoch 461/500\n",
      "993/993 [==============================] - 0s 171us/step - loss: 2.2498 - acc: 0.2890\n",
      "Epoch 462/500\n",
      "993/993 [==============================] - 0s 159us/step - loss: 2.2482 - acc: 0.3001\n",
      "Epoch 463/500\n",
      "993/993 [==============================] - 0s 174us/step - loss: 2.2479 - acc: 0.2941\n",
      "Epoch 464/500\n",
      "993/993 [==============================] - 0s 157us/step - loss: 2.2478 - acc: 0.2920\n",
      "Epoch 465/500\n",
      "993/993 [==============================] - 0s 171us/step - loss: 2.2461 - acc: 0.2951\n",
      "Epoch 466/500\n",
      "993/993 [==============================] - 0s 140us/step - loss: 2.2457 - acc: 0.2951\n",
      "Epoch 467/500\n",
      "993/993 [==============================] - 0s 139us/step - loss: 2.2455 - acc: 0.2900\n",
      "Epoch 468/500\n",
      "993/993 [==============================] - 0s 162us/step - loss: 2.2448 - acc: 0.3061\n",
      "Epoch 469/500\n",
      "993/993 [==============================] - 0s 158us/step - loss: 2.2453 - acc: 0.3031\n",
      "Epoch 470/500\n",
      "993/993 [==============================] - 0s 150us/step - loss: 2.2443 - acc: 0.2961\n",
      "Epoch 471/500\n",
      "993/993 [==============================] - 0s 141us/step - loss: 2.2427 - acc: 0.2951\n",
      "Epoch 472/500\n",
      "993/993 [==============================] - 0s 156us/step - loss: 2.2438 - acc: 0.2971\n",
      "Epoch 473/500\n",
      "993/993 [==============================] - 0s 147us/step - loss: 2.2431 - acc: 0.2971\n",
      "Epoch 474/500\n",
      "993/993 [==============================] - 0s 138us/step - loss: 2.2426 - acc: 0.2981\n",
      "Epoch 475/500\n",
      "993/993 [==============================] - 0s 133us/step - loss: 2.2422 - acc: 0.3011\n",
      "Epoch 476/500\n",
      "993/993 [==============================] - 0s 159us/step - loss: 2.2409 - acc: 0.3021\n",
      "Epoch 477/500\n",
      "993/993 [==============================] - 0s 154us/step - loss: 2.2402 - acc: 0.2951\n",
      "Epoch 478/500\n",
      "993/993 [==============================] - 0s 138us/step - loss: 2.2398 - acc: 0.2981\n",
      "Epoch 479/500\n",
      "993/993 [==============================] - 0s 175us/step - loss: 2.2395 - acc: 0.2961\n",
      "Epoch 480/500\n",
      "993/993 [==============================] - 0s 161us/step - loss: 2.2384 - acc: 0.2910\n",
      "Epoch 481/500\n",
      "993/993 [==============================] - 0s 151us/step - loss: 2.2392 - acc: 0.2991\n",
      "Epoch 482/500\n",
      "993/993 [==============================] - 0s 159us/step - loss: 2.2378 - acc: 0.2971\n",
      "Epoch 483/500\n",
      "993/993 [==============================] - 0s 131us/step - loss: 2.2366 - acc: 0.3001\n",
      "Epoch 484/500\n",
      "993/993 [==============================] - 0s 139us/step - loss: 2.2369 - acc: 0.3021\n",
      "Epoch 485/500\n",
      "993/993 [==============================] - 0s 138us/step - loss: 2.2365 - acc: 0.3011\n",
      "Epoch 486/500\n",
      "993/993 [==============================] - 0s 147us/step - loss: 2.2370 - acc: 0.2991\n",
      "Epoch 487/500\n",
      "993/993 [==============================] - 0s 153us/step - loss: 2.2352 - acc: 0.3031\n",
      "Epoch 488/500\n",
      "993/993 [==============================] - 0s 150us/step - loss: 2.2355 - acc: 0.2931\n",
      "Epoch 489/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993/993 [==============================] - 0s 146us/step - loss: 2.2340 - acc: 0.3001\n",
      "Epoch 490/500\n",
      "993/993 [==============================] - 0s 135us/step - loss: 2.2346 - acc: 0.2910\n",
      "Epoch 491/500\n",
      "993/993 [==============================] - 0s 133us/step - loss: 2.2353 - acc: 0.2931\n",
      "Epoch 492/500\n",
      "993/993 [==============================] - 0s 128us/step - loss: 2.2332 - acc: 0.2941\n",
      "Epoch 493/500\n",
      "993/993 [==============================] - 0s 130us/step - loss: 2.2330 - acc: 0.2981\n",
      "Epoch 494/500\n",
      "993/993 [==============================] - 0s 137us/step - loss: 2.2319 - acc: 0.2981\n",
      "Epoch 495/500\n",
      "993/993 [==============================] - 0s 148us/step - loss: 2.2327 - acc: 0.2890\n",
      "Epoch 496/500\n",
      "993/993 [==============================] - 0s 128us/step - loss: 2.2314 - acc: 0.3001\n",
      "Epoch 497/500\n",
      "993/993 [==============================] - 0s 137us/step - loss: 2.2303 - acc: 0.3001\n",
      "Epoch 498/500\n",
      "993/993 [==============================] - 0s 140us/step - loss: 2.2303 - acc: 0.2991\n",
      "Epoch 499/500\n",
      "993/993 [==============================] - 0s 133us/step - loss: 2.2302 - acc: 0.2941\n",
      "Epoch 500/500\n",
      "993/993 [==============================] - 0s 143us/step - loss: 2.2284 - acc: 0.2981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x222e2e70400>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelNN.fit(x_enc, y_enc,  batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_29\n",
      "2 290\n",
      "0 [-0.88929856 -0.62975734  2.322333    2.2121503  -0.24275294 -1.2494944 ]\n",
      "1 [ 0.75201565  0.04096923  3.6166577  -0.862813    3.2255642   3.0097854 ]\n",
      "2 [-0.29101992  2.0308578   0.93633074 -1.6293656  -0.6627589   2.6122117 ]\n",
      "3 [-0.14685006  3.8080506   0.58657104 -0.58574474 -1.1792847   1.1342607 ]\n",
      "4 [ 3.6231878   4.9606915   0.28368974  0.59521115 -1.088758    0.2781875 ]\n",
      "5 [1.0971725  4.8936467  0.86028063 3.7351582  1.6322545  0.2734988 ]\n",
      "6 [-0.21463598 -1.0257059  -1.0212944   2.3307908   1.0270495  -1.5242561 ]\n",
      "7 [ 3.0520196  -1.6872501  -1.4783219   3.2657852   1.6561127  -0.34780368]\n",
      "8 [ 2.8322997 -0.3983371 -1.2302314  0.2859466  5.3976007 -0.7820187]\n",
      "9 [-0.83763653 -0.07642663 -0.31995726 -0.39942712  2.1646383   0.21640038]\n",
      "10 [ 1.8320295  -0.38855258 -0.01479468  0.05653698  0.01409547  0.23713581]\n",
      "11 [ 0.10100773 -0.05692254  0.13033839 -0.03017608  0.10222936 -0.12685558]\n",
      "12 [-1.786692  -1.4301419  4.3195915 -1.4321048  1.6624048 -1.0742301]\n",
      "13 [-1.7167526   4.802023    0.7674255  -1.4956298   2.8104017   0.43623257]\n",
      "14 [-1.355362    2.9195118  -0.4437156  -0.37920654  1.2292529  -1.3293561 ]\n",
      "15 [-0.24115524 -1.0340374   1.9065801   1.0693651   0.5778853   5.5472035 ]\n",
      "16 [-0.16638301 -1.5498543  -0.25826374  0.24770536 -0.43664497  2.0261588 ]\n",
      "17 [-0.93820727 -1.8734579  -0.5458248   2.0084894   3.825347    0.5396018 ]\n",
      "18 [ 0.9327205  -1.8306663  -1.0781221  -0.41344702  1.7052569  -0.37389767]\n",
      "19 [-1.4562061  -1.8847001  -1.312227    0.15236649  1.3336675  -0.25281554]\n",
      "20 [-1.8099546 -1.5242323 -1.6463941  3.2726998 -0.9130348 -0.6645558]\n",
      "21 [ 0.88777083 -0.04089075 -0.7682027   4.8745513  -0.6777085  -0.60860956]\n",
      "22 [ 3.628153   -0.9059312  -0.9730904   4.5895605  -0.80555516  0.91995305]\n",
      "23 [ 2.5880845  -1.2008163  -1.2709117  -0.27527294  1.8034806   5.268318  ]\n",
      "24 [-0.2355571  -0.48432896  0.5490458  -1.0565406   1.7233206  -0.02500531]\n",
      "25 [ 2.3217285  -1.1420107   0.4046514  -0.8992606   0.67125654 -0.51030755]\n",
      "26 [-0.5030964   1.5255308   0.18501419  5.022547   -0.9296058   0.48714477]\n",
      "27 [ 0.8051956 -1.5276556  2.8169715  2.395709   1.7522814 -1.1311232]\n",
      "28 [ 1.3474852  -1.4852525   2.2090771   0.96296966  4.6376967  -0.814424  ]\n",
      "29 [-1.4155276  -1.1210932   4.186681   -0.7371202   2.088867   -0.86781275]\n",
      "30 [-1.3286699 -1.8761723  3.9589167 -1.8778187  1.3461659  3.97403  ]\n",
      "31 [ 4.642926   -1.1178465  -0.03831798  0.6430125   1.036378   -2.006113  ]\n",
      "32 [ 2.57706    -0.18338877  1.1112689  -1.8891524   4.4016504  -1.2035848 ]\n",
      "33 [ 3.249905    0.7557816   0.6643795  -1.5689619   0.87250996  0.2233121 ]\n",
      "34 [ 1.6595948  -0.7932793   5.9089046   0.25280076  0.8369776   0.15173829]\n",
      "35 [ 2.6711102   0.9026187   3.6162033  -1.8773097  -0.53715515 -1.5522364 ]\n",
      "36 [ 0.99120647  2.6420007   4.1899333   3.2629437   0.01961862 -0.9149337 ]\n",
      "37 [-0.85694695  1.6827711   2.6640747   2.0796232  -1.2399662   0.39613947]\n",
      "38 [-1.6668216  -1.8805318  -0.43788415  1.3772877   5.121766   -2.0226517 ]\n",
      "39 [-0.8425099 -1.7088743  5.165048  -1.6514366  1.4276294 -2.020156 ]\n",
      "40 [ 0.65511745  0.6069292   4.481158   -0.25953546 -0.7791668  -2.0159576 ]\n",
      "41 [-0.8191862  2.2546322  3.337978   1.6253161 -1.7434744 -1.1248596]\n",
      "42 [ 0.13862805 -0.04068562  0.11887331  0.04828373 -0.06824583  0.01978619]\n",
      "43 [ 0.62724596 -0.78428286  0.45178235  0.5333901   3.0353115   0.47366625]\n",
      "44 [ 1.5439961   1.9581466  -0.36232767 -0.97537744  3.6469495  -0.18458131]\n",
      "45 [ 2.7011583   3.645389   -1.1888646  -0.48659858  1.6414944   0.07358939]\n",
      "46 [ 2.8142204  1.0309372 -1.71735    2.1616824  5.50828    1.7954346]\n",
      "47 [ 0.11937231  1.38801    -0.837551    1.1916971   2.981965    1.1274693 ]\n",
      "48 [ 0.17400303  2.894273   -1.6173676   5.676484    1.3223017   2.228329  ]\n",
      "49 [1.1090361  4.8893523  0.85666746 3.7309551  1.646979   0.22529171]\n",
      "50 [-1.4206517  1.1242794  2.0859013  3.7184892  1.5411409 -1.6514412]\n",
      "51 [-8.5870767e-01  2.2610445e+00 -1.2784097e+00  2.1288190e+00\n",
      "  1.6114633e-03 -1.3886535e+00]\n",
      "52 [ 0.84302706  0.9044731   2.0317917   3.5232306   3.5207615  -0.19817922]\n",
      "53 [-1.2609506  2.9938488 -1.0625939  2.9552836  2.4128134  0.7812303]\n",
      "54 [-0.80256146  2.259493   -1.7819644   2.1764526   0.44950828  2.4566042 ]\n",
      "55 [ 1.579772   4.350216  -1.8551581  1.2421173 -1.7983862  2.2546954]\n",
      "56 [-0.3668277  2.7356741 -1.4374547  1.7962004 -1.4447037 -0.977926 ]\n",
      "57 [ 3.670732   2.8938968 -1.9310122  4.1969028 -1.4621788 -2.0119483]\n",
      "58 [-0.01925687 -0.02234355  0.0768133   0.06573071  0.11409025  0.07420462]\n",
      "59 [ 0.847716   1.6666782  0.6340933  3.161603   2.4815645 -1.5568064]\n",
      "60 [ 0.10375101 -0.4653772  -0.40152612  1.0585209   5.7355537   1.4019907 ]\n",
      "61 [ 1.3163439  -1.7358718   0.9213094   0.08118141  2.5760036   3.9125323 ]\n",
      "62 [ 0.35998982 -1.7719442   3.8265784  -1.1151651   0.73854005  2.356507  ]\n",
      "63 [-0.9490715  -1.8930485   5.1022334   0.98753333 -1.0328481   0.43613485]\n",
      "64 [ 0.35183114 -1.7660114   3.5186474   2.7958746  -1.1370714   3.15847   ]\n",
      "65 [ 5.0510416  -1.319322    1.5613202   2.8175144  -1.1704845   0.09948651]\n",
      "66 [ 0.40738454 -0.36986116  0.14184573 -0.06472902 -0.56160134 -0.65746546]\n",
      "67 [ 0.34076473  3.2220123   3.9483423   0.687307   -0.6049308  -0.6614296 ]\n",
      "68 [ 0.16523786 -0.22646733  0.09150489  0.8233862   4.9271073   1.1966131 ]\n",
      "69 [-0.42605916 -1.6622688   0.81521493 -0.48008117  3.7779837   0.6098461 ]\n",
      "70 [0.59880215 1.0592366  0.75162244 0.7891604  1.4225353  1.1344512 ]\n",
      "71 [ 3.6374514 -1.8757228 -1.9176068  4.626043  -0.4131442 -2.0060012]\n",
      "72 [-0.03482886 -0.06961491 -0.02560595 -0.09747153 -0.07900207  0.06706282]\n",
      "73 [-0.6091615 -1.435136   4.482447   4.7588215 -0.9737898 -2.0157435]\n",
      "74 [ 0.49999344 -1.8733894   0.62603855  1.0898684  -1.7236236  -0.6590789 ]\n",
      "75 [ 2.284663   -1.8824348  -0.98159266  0.05275499 -1.6851108   4.6917243 ]\n",
      "76 [-0.07337427 -0.02129684 -0.03560419  0.0647715   0.10642189 -0.07967587]\n",
      "77 [ 1.1961111   2.2179203   1.9006885   1.5145854  -0.07801771  1.8946398 ]\n",
      "78 [-1.0359966  -0.43131253  1.9875116  -0.9683823  -1.6162851   1.5704303 ]\n",
      "79 [-0.10355971 -0.5904099   0.32225636  1.6815239   3.0360632   1.0886323 ]\n",
      "80 [ 0.47041818  1.9651746   0.00341326  2.7253711   3.2181683  -1.4144214 ]\n",
      "81 [ 0.72725654 -1.8751619   2.2283344   0.71724206  3.9505916  -1.8556974 ]\n",
      "82 [-0.45108086 -1.7554698   3.4941602  -0.02878506  4.283849   -1.3604091 ]\n",
      "83 [-0.15224403  4.784252    0.99641377 -1.2910923   2.1880217  -1.5098879 ]\n",
      "84 [-1.0707557   0.87442493  1.0498714   2.6603425   4.7372794  -0.99245894]\n",
      "85 [-1.0471361  -0.5423304   0.96575594  0.7723512   6.3429785  -0.24071786]\n",
      "86 [1.9855015  0.20320293 2.7481499  1.0878385  3.4955273  1.0616665 ]\n",
      "87 [ 0.7707155   0.42355055 -0.42766008  4.5720997   1.4439294  -0.5273302 ]\n",
      "88 [ 1.1745477   1.5598025   2.3767986   4.2712502  -1.3567998   0.36881062]\n",
      "89 [-1.2841903  2.6315608  4.8278594  0.5100992 -0.8314391  3.3099604]\n",
      "90 [ 0.31441185  4.0519595  -0.31683487  0.69481844 -1.6497545   4.26013   ]\n",
      "91 [ 2.3399131   2.4456081   0.3796169   0.66034037 -1.9383959   0.84941673]\n",
      "92 [ 0.11896051 -0.03300998  0.1260847  -0.08723997 -0.13456136  0.12667535]\n",
      "93 [ 0.68792146 -1.4048145   0.80633974  4.4387064  -1.911334    0.5541509 ]\n",
      "94 [ 3.260425   -1.8808091  -1.7412771  -1.0190511   0.75997365  4.8326173 ]\n",
      "95 [ 0.02279976 -0.01737207 -0.13650078 -0.13903728 -0.0193733   0.07084581]\n",
      "96 [ 1.7124869   1.4837137   0.4723762   4.684141   -0.49326926 -0.91110903]\n",
      "97 [ 5.2669053   0.59300834  3.1737535  -1.4099016  -1.4258623  -0.17645577]\n",
      "98 [ 4.173622    0.61048067  0.54136395 -1.6045498  -1.120472    1.442023  ]\n",
      "99 [ 4.1792579e+00  6.8956000e-01  5.7071602e-01  9.1763914e-01\n",
      " -4.5798125e-04  1.0573590e+00]\n",
      "100 [1.42687   0.7183692 2.5798802 4.801088  0.5696792 0.834992 ]\n",
      "101 [ 2.9793549   3.4548297   2.089263   -0.27567956  1.3572071   0.35105053]\n",
      "102 [ 0.3093656   1.38183    -1.3297515  -1.6447643   0.16024983 -0.39543575]\n",
      "103 [ 2.0147498   3.1835005  -1.577108   -0.58519655  6.1259494   1.805676  ]\n",
      "104 [-1.6993191  2.3706963  1.1356888  0.5058053  2.9356506  3.3343334]\n",
      "105 [-0.8951988  -1.1669585   2.7218282   4.419551    0.49737892  3.5276635 ]\n",
      "106 [ 2.1619565  0.7699466  1.894341   5.4127965 -0.8803712  2.9001987]\n",
      "107 [ 5.1300263   0.887073   -0.23223181 -1.0085262  -1.3018514  -1.0569165 ]\n",
      "108 [ 4.0953646   1.7467865   1.4030881  -0.95633966 -1.6744778   0.4788888 ]\n",
      "109 [ 3.4954784  3.1636539 -0.3943835 -1.8040252  1.9593409  2.3114016]\n",
      "110 [-1.4748743e-03  7.5328630e-01  4.4097546e-01  4.9081168e+00\n",
      " -9.9449748e-01  7.6361758e-01]\n",
      "111 [ 0.6615894 -1.4055558  0.8228173  4.371112  -1.9253308  0.5420826]\n",
      "112 [-0.01412097 -0.07149995  0.04471123  0.06098424 -0.01635051  0.02654663]\n",
      "113 [-1.3743224 -1.2803272  1.934507   1.3938638  2.1244445  5.7001967]\n",
      "114 [ 2.7332485   0.06642235 -0.16136941  1.7880349  -0.18097952  2.0989957 ]\n",
      "115 [ 0.76654154  1.2327865   0.7248113   4.786665   -0.9311838  -0.23724337]\n",
      "116 [-1.5644326   1.6736116   4.049169   -0.07180936 -0.6367751  -1.7520374 ]\n",
      "117 [-1.6888103   2.7997756   0.75151193 -1.5604949   2.6386044  -0.82223713]\n",
      "118 [-1.8271985   0.51839423 -0.7755766  -1.2000834   3.7309191  -1.8505132 ]\n",
      "119 [-1.8294145 -1.0500716 -0.6075266 -1.8075081  4.050779  -2.0117216]\n",
      "120 [-1.2382667  -0.38211432  1.7798631  -1.8796954   3.416309   -1.7617683 ]\n",
      "121 [-1.1976403   0.55865204  2.968914   -1.885255    2.153361   -1.1757052 ]\n",
      "122 [-1.7322198  3.5440323  1.5914669 -1.6077653  2.2760642 -1.8922018]\n",
      "123 [ 1.5450221   1.1907617  -0.34496665  0.39360827  5.905131   -0.27911225]\n",
      "124 [-1.6745714 -1.658083   2.7694662 -1.2672063  1.6114217  1.622199 ]\n",
      "125 [ 0.04028897  0.05218022 -0.01536132 -0.0420787   0.08720772 -0.10150349]\n",
      "126 [-0.29240987  0.12494203 -1.1691555   0.361994    1.4986418   0.18187086]\n",
      "127 [-0.07570471 -1.0065132   0.8179592   0.9106383   1.1191276   5.8291306 ]\n",
      "128 [-0.7938878 -1.8858032 -1.068844  -0.8471806  2.3870962  5.303271 ]\n",
      "129 [-1.7805718  -1.8778851  -1.4129717  -0.77530617  2.138989    3.8955264 ]\n",
      "130 [-1.8776037  -1.8822682  -0.41209805  0.49899614 -0.72771186  4.4537992 ]\n",
      "131 [-1.8509588 -1.8204426  1.4528135 -0.8758099 -1.5987787  4.4394956]\n",
      "132 [-1.8618234 -1.360126   3.5510917 -1.7191834 -1.6666019  1.7056241]\n",
      "133 [-1.8635325  -1.1713902   2.0284388  -1.8876023  -0.09052475  0.59270793]\n",
      "134 [-1.867598   -0.28140217 -0.91227764 -1.8768009   3.4236243   0.4896128 ]\n",
      "135 [-1.8558365  -0.85766345 -1.9187434  -1.4006355   0.9478969   2.133413  ]\n",
      "136 [-0.5635059 -1.4928544 -1.9236022  1.5505583 -0.7359271  3.889753 ]\n",
      "137 [ 3.713079   -0.72559357 -1.6084608  -0.09856493 -1.9268438   2.5552506 ]\n",
      "138 [ 2.8989198 -0.3269342  1.0019549  0.5875439 -1.9250724 -0.6388432]\n",
      "139 [ 3.7939315  -1.4756129   4.6220536   0.98319995 -1.8883662   0.8210653 ]\n",
      "140 [ 4.176261  -1.6188701 -0.1760364  4.764104  -0.5509686  3.0864954]\n",
      "141 [ 1.5503793  0.4710167 -0.9881317  1.5222387  6.853099   2.021048 ]\n",
      "142 [ 0.10002775 -1.1419897  -0.06864723  1.8451017   2.0261288   2.2012908 ]\n",
      "143 [ 0.8715462   0.8971157   2.0330033   3.5417879   3.502784   -0.19977859]\n",
      "144 [ 1.4428539  1.335603  -1.0082899 -0.9617406  1.8415266  4.6607995]\n",
      "145 [ 0.5721528  -1.3177291  -0.9447263   0.16127583 -0.47467884 -0.30621424]\n",
      "146 [-0.44842353  0.8370958  -0.6256367  -0.06087337  1.7227426   5.5275693 ]\n",
      "147 [ 1.4566561 -1.5264933 -1.5246689  3.8593276  1.197906   2.7827766]\n",
      "148 [ 1.8756534  -0.80716395  0.09692323  0.7376083   1.1978621   4.0836315 ]\n",
      "149 [-1.3383554   0.32054526  4.7043424   0.8566232  -0.76100004  2.6256979 ]\n",
      "150 [-1.5735909 -0.5288932  1.8949534  4.1422567 -1.9348334 -1.3445921]\n",
      "151 [ 2.9348798 -0.7763889  0.3616855  3.0766947 -1.6621401 -1.4641801]\n",
      "152 [ 3.910623   -0.00461434  2.0038745   2.9702685  -1.2150629  -1.0987375 ]\n",
      "153 [ 0.45544517 -1.3415152   4.107923   -0.76129466 -1.7745339  -1.3599263 ]\n",
      "154 [ 3.0666988 -1.8576835  3.6518998 -1.523687  -1.3019388 -1.3463684]\n",
      "155 [ 0.5285806  -1.0492688   3.6311538  -0.79620844 -1.1366875   0.6203785 ]\n",
      "156 [-1.3622794  -0.5458794   0.97288203 -1.7248068  -0.8305501   0.47569257]\n",
      "157 [-1.0041541  -0.25084442 -0.72552705 -1.0394777  -1.1496707   3.5079908 ]\n",
      "158 [ 2.3869834   0.24230397  2.4266214   0.7391798  -1.1390008   2.7740521 ]\n",
      "159 [ 0.4061803   4.812125    1.2862304  -0.15068035 -1.4809227  -0.74666995]\n",
      "160 [-1.3648452   2.764365   -0.62407786  2.9683504   0.55399805 -0.19182554]\n",
      "161 [-1.7951596  1.3406601 -0.5625305  2.2982316 -0.9196111  1.7986416]\n",
      "162 [-1.8568189   4.1827073   3.5255218   3.2317085   1.2757478   0.11266783]\n",
      "163 [-0.4919592  1.1229988  1.1442343  2.812025   3.3289845 -1.655493 ]\n",
      "164 [ 1.8528373 -1.6059222 -1.9385537  3.7256732  4.2625365 -2.0119748]\n",
      "165 [ 0.14193214 -0.07361897  0.02229138 -0.0355548   0.09314333  0.00341581]\n",
      "166 [-0.6786682  -1.7444918   4.835296    0.24071385  2.2684674   2.3935049 ]\n",
      "167 [ 1.2625009  -0.9048503   3.5262115  -1.0369971  -0.47205833  3.3827415 ]\n",
      "168 [-0.74821335 -0.6790601  -1.3631583   4.616628   -0.60049736  4.5693326 ]\n",
      "169 [ 0.8665454  -0.59953433 -0.25642505  1.2344637   0.16477604  2.5708907 ]\n",
      "170 [-1.8504157  -1.2933012   4.9282217  -0.49778095 -1.6184732  -1.58641   ]\n",
      "171 [-1.8633355  -0.41033253  1.0151637  -0.15536453  2.0885668  -2.0217786 ]\n",
      "172 [ 0.6075862  -0.14444572  1.272974    1.102656    4.9334445  -1.5669805 ]\n",
      "173 [ 0.50320876 -0.4718104  -0.08933163  0.43131214  5.6729693   1.3977958 ]\n",
      "174 [-0.44101158 -0.84419423 -1.5485841  -1.5365654   4.8446436  -0.47279584]\n",
      "175 [-0.6804825  -0.6465572  -1.9348975  -0.76051426  5.1093073   1.0431439 ]\n",
      "176 [-1.2943074   1.808557   -0.91758126  2.8196342   2.481886    5.840079  ]\n",
      "177 [ 0.20072755 -0.82103467 -0.93660223  1.8192717   3.7936926   4.548078  ]\n",
      "178 [-0.53346825  2.9335086   0.20832865  5.097624   -0.98415685 -0.02344025]\n",
      "179 [ 3.531597   -0.63879156  2.165867    0.6469651   1.5958263   1.651161  ]\n",
      "180 [ 2.8060136  -1.5229566  -0.88071585  4.748491    2.2902064  -0.527879  ]\n",
      "181 [-1.3628936   2.7725644  -0.6217639   2.9807227   0.5572205  -0.18873452]\n",
      "182 [ 2.9626434  -0.48327324  3.6956365   0.03279193  1.0232948  -1.1689906 ]\n",
      "183 [ 4.097899   -0.71883774 -0.5905643   0.17165266  2.7992058   1.1799341 ]\n",
      "184 [1.6712804 0.2528041 2.5708468 1.9629517 1.8488575 0.9062323]\n",
      "185 [-0.65302193  1.6491587  -0.5980548  -1.2265527   0.16418493  0.28317794]\n",
      "186 [ 0.8856711  2.6962972  5.961171   1.2485353 -1.3498981  3.587775 ]\n",
      "187 [ 0.15233374  4.0194435   0.8675261   3.4092693  -1.6825161   1.5846897 ]\n",
      "188 [ 0.5875763  2.6718922  2.2653706  1.6958508 -1.562608   1.6612571]\n",
      "189 [ 0.07901341 -0.03048456  0.02950275 -0.00859366  0.12117656 -0.06517252]\n",
      "190 [ 1.6326241   2.7690022   0.89921784 -1.3086857   2.3124328   3.939451  ]\n",
      "191 [-1.679049    4.035259    0.96974653 -0.8604395  -0.5358435  -0.5233184 ]\n",
      "192 [-1.0510162  2.703593  -1.71943   -1.4481201 -1.7720168  1.867858 ]\n",
      "193 [ 3.5405314  -0.39934266 -1.8643368  -1.4656936   0.282043    1.8169999 ]\n",
      "194 [ 4.130091   -0.71646917 -0.58840185  0.16453125  2.7913089   1.1740983 ]\n",
      "195 [ 3.7488513   2.1736887  -1.9223261  -0.93657565  3.1492815  -0.14267984]\n",
      "196 [ 2.3594508  4.181692  -1.510138   1.8809314  2.4808254  2.2424223]\n",
      "197 [ 3.725803   4.7034407 -0.4857329  1.0702953  1.1511532 -1.6042395]\n",
      "198 [ 1.7986712   0.7295129   1.9382637  -1.0028789   2.8126261  -0.39549848]\n",
      "199 [ 1.7535573  1.4354885  2.9538832 -1.1516571 -0.4793949  1.3895215]\n",
      "200 [ 0.51554143  4.4514503   0.01681666  3.6754992  -0.7352887  -1.0280267 ]\n",
      "201 [ 3.1195288   2.0423431  -0.24973877  3.2159154  -0.43691528 -1.337869  ]\n",
      "202 [-0.03613399  0.0580198   0.02752756 -0.08513927 -0.08308613 -0.02407567]\n",
      "203 [ 0.08127129  0.10882472  0.05619447 -0.07724605 -0.02654643 -0.01011364]\n",
      "204 [-1.8605076   0.20790339  0.8580331   0.25616583  1.0653573   4.9722266 ]\n",
      "205 [-1.0486443  -0.72848547  0.74505866  4.155081   -0.70258164  5.254126  ]\n",
      "206 [ 3.3234663   4.30594    -1.6135784  -0.01661145 -1.4432403   0.79488873]\n",
      "207 [ 1.5234846   0.4605727  -1.7127352   1.1483753  -0.65173036 -1.2044693 ]\n",
      "208 [ 1.1028953  -0.9312772  -1.8428187   1.2606745   0.30414355  0.19380479]\n",
      "209 [ 4.1108365  -1.4001502  -1.8575813   0.50726545 -1.8930452   0.0166749 ]\n",
      "210 [-0.63978356  2.8025792   4.313328    2.8522978  -1.8218298  -1.0740772 ]\n",
      "211 [-1.1749985 -1.1804571  3.7300165  4.989669   0.5852487  7.013264 ]\n",
      "212 [ 4.2367663  -1.8754493   2.8907723  -0.12111565  1.146037   -1.1977675 ]\n",
      "213 [-1.8469892   1.8124803  -0.76874024 -1.8968409  -1.4838175   3.7958796 ]\n",
      "214 [-1.7594861  3.5500255  2.1636188 -1.8767753  0.2791125  0.8195032]\n",
      "215 [-1.8484708   4.1124334   1.3329966  -1.6707945  -0.18063372 -2.0124397 ]\n",
      "216 [-0.84397906  4.810002    2.374495   -0.14887343 -0.69172186 -1.6348522 ]\n",
      "217 [-1.8520216   0.49734136 -0.68750674  4.8896737  -0.9396202  -2.0031216 ]\n",
      "218 [ 0.5031186   4.139583   -1.7609766   0.529243   -0.64277905  0.9246386 ]\n",
      "219 [ 0.09563296  3.920954   -1.9219575   1.8548024  -1.4411491   3.3225577 ]\n",
      "220 [-1.744759    4.44132    -1.9318393   3.0473974   0.9501409  -0.92608637]\n",
      "221 [-1.8553288  2.0755813 -0.7092974  2.7729664  1.0870383  1.5782559]\n",
      "222 [-1.4831679  -1.0592766   2.1052225   0.93540037  0.52094793  5.6173425 ]\n",
      "223 [ 2.7148917   0.04959653  0.11283787  0.7603482  -1.3920126   5.6913676 ]\n",
      "224 [ 3.7750747   2.3778875  -1.2987636   0.85269725 -1.702169    1.0879098 ]\n",
      "225 [-1.6164564  4.5753455 -1.6705397  1.1140574 -1.683714  -1.7201054]\n",
      "226 [-1.3687885   4.020933   -0.15660144  0.26823875 -1.7314966   0.7908311 ]\n",
      "227 [ 3.1319542  5.232603   1.9562193 -1.7262204 -1.1190574 -2.0078182]\n",
      "228 [ 3.1487439   4.841788    0.39413482 -0.7870431   0.29573146 -0.46546358]\n",
      "229 [ 0.47353607  4.8040347   0.55573565 -0.6043818   0.23564343 -0.81432945]\n",
      "230 [ 0.5260472  0.1726738 -0.9120685  1.8743368  1.5463041  2.530166 ]\n",
      "231 [-1.082096   -0.94103193  0.66353124  1.947762    0.13317579  5.212103  ]\n",
      "232 [ 0.13117962  0.10991932 -0.13667087  0.01336087 -0.09876471 -0.09012175]\n",
      "233 [ 4.6476445   1.5662097   1.1157631   2.6601963  -0.68197006 -2.0031598 ]\n",
      "234 [-0.6409228  2.7864122  4.284117   2.8257155 -1.8227853 -1.0794448]\n",
      "235 [ 2.406038    1.6681933   0.68642837 -0.9015621   2.854525   -1.3187732 ]\n",
      "236 [ 0.02952227  4.8506875   1.044251    1.773336   -0.22735167 -2.007531  ]\n",
      "237 [ 0.22680834 -0.60316235 -0.27192745  0.95753706  5.5101953   1.5676754 ]\n",
      "238 [-0.91093355 -0.74923503  0.18380874  1.5578573   0.04391773  5.1730733 ]\n",
      "239 [ 2.1446378 -1.8744501  0.6660512  4.950181   1.4294279  1.6534604]\n",
      "240 [ 0.83711684  0.8738743   2.0480084   3.5544095   3.447834   -0.21313064]\n",
      "241 [-0.01291728  0.10629687  0.03657955 -0.07828292  0.06380697 -0.09951285]\n",
      "242 [ 0.9104299   4.8992558  -1.3861363   0.06280704  3.0376244   3.850459  ]\n",
      "243 [ 3.9755576   0.42072535 -1.1252807   0.6029728   3.0016894  -0.2626824 ]\n",
      "244 [ 1.6292893  -0.61389863  2.445099    0.8903192   3.326932    2.122023  ]\n",
      "245 [-1.7564422   2.0311594  -1.677827    0.48140934  4.1666594  -1.9803694 ]\n",
      "246 [-1.8482788 -0.4110169 -1.9149213  2.7905738  2.5244095 -1.9924661]\n",
      "247 [-1.8597555  -1.5513743  -1.9314057   0.65504897  2.7480075  -1.5872087 ]\n",
      "248 [-1.8628272  -1.8094316  -1.9233085   0.0382506   4.211312   -0.40758044]\n",
      "249 [ 0.00910807 -0.03670707  0.0549929  -0.07077391  0.00471231 -0.11768984]\n",
      "250 [ 0.21021473 -1.2409842  -1.8574728  -1.398471    4.46784    -2.0136735 ]\n",
      "251 [ 2.1279252  -0.6423324  -0.43087408 -1.4724115   2.7927816  -2.0177968 ]\n",
      "252 [ 5.971085   0.9229864 -0.540786  -1.3995632  0.8365961 -1.2665614]\n",
      "253 [-0.5834501   0.35109538  1.3600665   1.8124338   3.441913   -1.1146948 ]\n",
      "254 [-1.8761773  -0.8388453   0.42480847  3.4073622   4.5186186   3.8657293 ]\n",
      "255 [-0.10616106  0.02495779 -0.00503394 -0.04022801  0.10318755 -0.07799426]\n",
      "256 [ 0.86221397  0.7989276  -1.2200835   1.5147246  -1.5266353   2.6543925 ]\n",
      "257 [-0.1919862   0.29168847  2.8218563  -1.0924339  -1.8949699  -1.5721178 ]\n",
      "258 [ 0.7266645  -1.545821    2.6186693  -1.9001726  -0.94822174 -0.07711759]\n",
      "259 [-1.7924343   2.6034157   0.34326494 -1.778172   -1.6453868  -0.1975273 ]\n",
      "260 [-1.8563906   1.9851261  -0.695346   -0.21287161 -1.9349129  -0.71352714]\n",
      "261 [-1.7814729  -0.89279544 -1.8731271   1.9461691  -1.9447907   1.1289096 ]\n",
      "262 [-0.24031292 -1.505924   -1.7164487  -1.7733402  -1.9521871   4.933615  ]\n",
      "263 [ 3.5798645 -1.2526575 -0.5046723 -1.5333232 -1.3965526  4.13182  ]\n",
      "264 [ 0.330625   -0.8557212   0.91819036 -0.225958   -0.8147361   4.9346642 ]\n",
      "265 [-1.3580214  -0.93193555  0.05303869  4.786511    0.5359204   5.010693  ]\n",
      "266 [ 0.06304769 -0.11717276 -0.03213556 -0.01498447 -0.05281016  0.13003053]\n",
      "267 [ 1.6816587  -1.8804735   1.6015111  -1.8398417   3.6363075   0.40019882]\n",
      "268 [ 4.3813667 -1.1221733  3.0860972 -1.1403251  2.7045178  1.1029946]\n",
      "269 [-1.1007189 -1.2171594  1.1918238  4.5344453  1.4956493  4.1896505]\n",
      "270 [ 4.594243   -1.8909129  -0.26308188 -1.4806795  -0.44443354  0.44657704]\n",
      "271 [ 1.9299093  -1.253859    0.22969131 -1.8864602  -0.6256598   1.4933566 ]\n",
      "272 [-0.987971  -0.9807945  0.6507921 -1.8798212 -1.521834   4.829522 ]\n",
      "273 [ 3.6737936   3.5275695  -0.98311234 -1.7232398   0.86668605  4.6939807 ]\n",
      "274 [ 1.1978236  4.749978   2.570828  -1.7469472  0.8542388 -1.0589801]\n",
      "275 [-0.4840236 -0.5867031  1.4833719  1.3191161 -0.8755026  5.521993 ]\n",
      "276 [ 0.7318418   0.0129352   3.5853689  -0.88753986  3.1556168   2.9310741 ]\n",
      "277 [-0.13787709 -0.11572106 -0.08929805  0.00093365  0.04825164  0.06654188]\n",
      "278 [ 0.31285426 -0.59902155 -0.35551074  0.74291945  5.655489    1.0743699 ]\n",
      "279 [-0.29669     4.078554   -1.4250909   0.99936986 -1.4887719   1.2458261 ]\n",
      "280 [ 0.06495576  1.6238012   2.174866    3.7113183   4.148494   -1.9840901 ]\n",
      "281 [ 2.1875317  -0.5081623   5.6698666  -0.58543533  0.63397855 -1.1665426 ]\n",
      "282 [ 1.2073262   4.7224693   2.5688236  -1.7478006   0.85162646 -1.063059  ]\n",
      "283 [ 0.74623644  0.03570469  3.6062105  -0.8716523   3.1954088   2.9648576 ]\n",
      "284 [ 1.1083938   4.2956777  -1.8285582   4.751915   -0.00899996  0.5764814 ]\n",
      "285 [ 1.078455    0.779054    0.8945711   5.0183754  -1.0918479   0.27984023]\n",
      "286 [ 1.4860893  -0.68693334  3.285673    0.21103711  2.048138   -1.9145039 ]\n",
      "287 [ 3.1398625   1.388847    0.46397182 -1.0013769   1.8969288  -0.6158497 ]\n",
      "288 [ 0.6608625  4.220581  -1.7306898 -1.8983806  1.4962322 -2.0165186]\n",
      "289 [-0.1356279  -0.05663224 -0.0357901   0.06276853 -0.00373261  0.06934156]\n"
     ]
    }
   ],
   "source": [
    "w2vLayer = modelNN.layers[0]\n",
    "print (w2vLayer.name)\n",
    "wts = w2vLayer.get_weights()\n",
    "#wts[0]== Weight, wts[1]= bias\n",
    "print (len(wts), len(wts[0]))\n",
    "for i,w in enumerate(wts[0]):\n",
    "    print (i, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 [-0.88929856 -0.62975734  2.322333    2.2121503  -0.24275294 -1.2494944 ]\n",
      "linguistics 1 [ 0.75201565  0.04096923  3.6166577  -0.862813    3.2255642   3.0097854 ]\n",
      "word 2 [-0.29101992  2.0308578   0.93633074 -1.6293656  -0.6627589   2.6122117 ]\n",
      "embeddings 3 [-0.14685006  3.8080506   0.58657104 -0.58574474 -1.1792847   1.1342607 ]\n",
      "were 4 [ 3.6231878   4.9606915   0.28368974  0.59521115 -1.088758    0.2781875 ]\n",
      "discussed 5 [1.0971725  4.8936467  0.86028063 3.7351582  1.6322545  0.2734988 ]\n",
      "the 6 [-0.21463598 -1.0257059  -1.0212944   2.3307908   1.0270495  -1.5242561 ]\n",
      "research 7 [ 3.0520196  -1.6872501  -1.4783219   3.2657852   1.6561127  -0.34780368]\n",
      "area 8 [ 2.8322997 -0.3983371 -1.2302314  0.2859466  5.3976007 -0.7820187]\n",
      "of 9 [-0.83763653 -0.07642663 -0.31995726 -0.39942712  2.1646383   0.21640038]\n",
      "distributional 10 [ 1.8320295  -0.38855258 -0.01479468  0.05653698  0.01409547  0.23713581]\n",
      "semantics 11 [ 0.10100773 -0.05692254  0.13033839 -0.03017608  0.10222936 -0.12685558]\n",
      "it 12 [-1.786692  -1.4301419  4.3195915 -1.4321048  1.6624048 -1.0742301]\n",
      "aims 13 [-1.7167526   4.802023    0.7674255  -1.4956298   2.8104017   0.43623257]\n",
      "to 14 [-1.355362    2.9195118  -0.4437156  -0.37920654  1.2292529  -1.3293561 ]\n",
      "quantify 15 [-0.24115524 -1.0340374   1.9065801   1.0693651   0.5778853   5.5472035 ]\n",
      "and 16 [-0.16638301 -1.5498543  -0.25826374  0.24770536 -0.43664497  2.0261588 ]\n",
      "categorize 17 [-0.93820727 -1.8734579  -0.5458248   2.0084894   3.825347    0.5396018 ]\n",
      "semantic 18 [ 0.9327205  -1.8306663  -1.0781221  -0.41344702  1.7052569  -0.37389767]\n",
      "similarities 19 [-1.4562061  -1.8847001  -1.312227    0.15236649  1.3336675  -0.25281554]\n",
      "between 20 [-1.8099546 -1.5242323 -1.6463941  3.2726998 -0.9130348 -0.6645558]\n",
      "linguistic 21 [ 0.88777083 -0.04089075 -0.7682027   4.8745513  -0.6777085  -0.60860956]\n",
      "items 22 [ 3.628153   -0.9059312  -0.9730904   4.5895605  -0.80555516  0.91995305]\n",
      "based 23 [ 2.5880845  -1.2008163  -1.2709117  -0.27527294  1.8034806   5.268318  ]\n",
      "on 24 [-0.2355571  -0.48432896  0.5490458  -1.0565406   1.7233206  -0.02500531]\n",
      "their 25 [ 2.3217285  -1.1420107   0.4046514  -0.8992606   0.67125654 -0.51030755]\n",
      "properties 26 [-0.5030964   1.5255308   0.18501419  5.022547   -0.9296058   0.48714477]\n",
      "large 27 [ 0.8051956 -1.5276556  2.8169715  2.395709   1.7522814 -1.1311232]\n",
      "samples 28 [ 1.3474852  -1.4852525   2.2090771   0.96296966  4.6376967  -0.814424  ]\n",
      "language 29 [-1.4155276  -1.1210932   4.186681   -0.7371202   2.088867   -0.86781275]\n",
      "data 30 [-1.3286699 -1.8761723  3.9589167 -1.8778187  1.3461659  3.97403  ]\n",
      "underlying 31 [ 4.642926   -1.1178465  -0.03831798  0.6430125   1.036378   -2.006113  ]\n",
      "idea 32 [ 2.57706    -0.18338877  1.1112689  -1.8891524   4.4016504  -1.2035848 ]\n",
      "that 33 [ 3.249905    0.7557816   0.6643795  -1.5689619   0.87250996  0.2233121 ]\n",
      "\"a 34 [ 1.6595948  -0.7932793   5.9089046   0.25280076  0.8369776   0.15173829]\n",
      "is 35 [ 2.6711102   0.9026187   3.6162033  -1.8773097  -0.53715515 -1.5522364 ]\n",
      "characterized 36 [ 0.99120647  2.6420007   4.1899333   3.2629437   0.01961862 -0.9149337 ]\n",
      "by 37 [-0.85694695  1.6827711   2.6640747   2.0796232  -1.2399662   0.39613947]\n",
      "company 38 [-1.6668216  -1.8805318  -0.43788415  1.3772877   5.121766   -2.0226517 ]\n",
      "keeps\" 39 [-0.8425099 -1.7088743  5.165048  -1.6514366  1.4276294 -2.020156 ]\n",
      "was 40 [ 0.65511745  0.6069292   4.481158   -0.25953546 -0.7791668  -2.0159576 ]\n",
      "popularized 41 [-0.8191862  2.2546322  3.337978   1.6253161 -1.7434744 -1.1248596]\n",
      "firth 42 [ 0.13862805 -0.04068562  0.11887331  0.04828373 -0.06824583  0.01978619]\n",
      "technique 43 [ 0.62724596 -0.78428286  0.45178235  0.5333901   3.0353115   0.47366625]\n",
      "representing 44 [ 1.5439961   1.9581466  -0.36232767 -0.97537744  3.6469495  -0.18458131]\n",
      "words 45 [ 2.7011583   3.645389   -1.1888646  -0.48659858  1.6414944   0.07358939]\n",
      "as 46 [ 2.8142204  1.0309372 -1.71735    2.1616824  5.50828    1.7954346]\n",
      "vectors 47 [ 0.11937231  1.38801    -0.837551    1.1916971   2.981965    1.1274693 ]\n",
      "has 48 [ 0.17400303  2.894273   -1.6173676   5.676484    1.3223017   2.228329  ]\n",
      "roots 49 [1.1090361  4.8893523  0.85666746 3.7309551  1.646979   0.22529171]\n",
      "1960s 50 [-1.4206517  1.1242794  2.0859013  3.7184892  1.5411409 -1.6514412]\n",
      "with 51 [-8.5870767e-01  2.2610445e+00 -1.2784097e+00  2.1288190e+00\n",
      "  1.6114633e-03 -1.3886535e+00]\n",
      "development 52 [ 0.84302706  0.9044731   2.0317917   3.5232306   3.5207615  -0.19817922]\n",
      "vector 53 [-1.2609506  2.9938488 -1.0625939  2.9552836  2.4128134  0.7812303]\n",
      "space 54 [-0.80256146  2.259493   -1.7819644   2.1764526   0.44950828  2.4566042 ]\n",
      "model 55 [ 1.579772   4.350216  -1.8551581  1.2421173 -1.7983862  2.2546954]\n",
      "for 56 [-0.3668277  2.7356741 -1.4374547  1.7962004 -1.4447037 -0.977926 ]\n",
      "information 57 [ 3.670732   2.8938968 -1.9310122  4.1969028 -1.4621788 -2.0119483]\n",
      "retrieval 58 [-0.01925687 -0.02234355  0.0768133   0.06573071  0.11409025  0.07420462]\n",
      "reducing 59 [ 0.847716   1.6666782  0.6340933  3.161603   2.4815645 -1.5568064]\n",
      "number 60 [ 0.10375101 -0.4653772  -0.40152612  1.0585209   5.7355537   1.4019907 ]\n",
      "dimensions 61 [ 1.3163439  -1.7358718   0.9213094   0.08118141  2.5760036   3.9125323 ]\n",
      "using 62 [ 0.35998982 -1.7719442   3.8265784  -1.1151651   0.73854005  2.356507  ]\n",
      "singular 63 [-0.9490715  -1.8930485   5.1022334   0.98753333 -1.0328481   0.43613485]\n",
      "value 64 [ 0.35183114 -1.7660114   3.5186474   2.7958746  -1.1370714   3.15847   ]\n",
      "decomposition 65 [ 5.0510416  -1.319322    1.5613202   2.8175144  -1.1704845   0.09948651]\n",
      "then 66 [ 0.40738454 -0.36986116  0.14184573 -0.06472902 -0.56160134 -0.65746546]\n",
      "led 67 [ 0.34076473  3.2220123   3.9483423   0.687307   -0.6049308  -0.6614296 ]\n",
      "introduction 68 [ 0.16523786 -0.22646733  0.09150489  0.8233862   4.9271073   1.1966131 ]\n",
      "latent 69 [-0.42605916 -1.6622688   0.81521493 -0.48008117  3.7779837   0.6098461 ]\n",
      "analysis 70 [0.59880215 1.0592366  0.75162244 0.7891604  1.4225353  1.1344512 ]\n",
      "late 71 [ 3.6374514 -1.8757228 -1.9176068  4.626043  -0.4131442 -2.0060012]\n",
      "1980s 72 [-0.03482886 -0.06961491 -0.02560595 -0.09747153 -0.07900207  0.06706282]\n",
      "2000 73 [-0.6091615 -1.435136   4.482447   4.7588215 -0.9737898 -2.0157435]\n",
      "bengio 74 [ 0.49999344 -1.8733894   0.62603855  1.0898684  -1.7236236  -0.6590789 ]\n",
      "et 75 [ 2.284663   -1.8824348  -0.98159266  0.05275499 -1.6851108   4.6917243 ]\n",
      "al 76 [-0.07337427 -0.02129684 -0.03560419  0.0647715   0.10642189 -0.07967587]\n",
      "provided 77 [ 1.1961111   2.2179203   1.9006885   1.5145854  -0.07801771  1.8946398 ]\n",
      "a 78 [-1.0359966  -0.43131253  1.9875116  -0.9683823  -1.6162851   1.5704303 ]\n",
      "series 79 [-0.10355971 -0.5904099   0.32225636  1.6815239   3.0360632   1.0886323 ]\n",
      "papers 80 [ 0.47041818  1.9651746   0.00341326  2.7253711   3.2181683  -1.4144214 ]\n",
      "\"neural 81 [ 0.72725654 -1.8751619   2.2283344   0.71724206  3.9505916  -1.8556974 ]\n",
      "probabilistic 82 [-0.45108086 -1.7554698   3.4941602  -0.02878506  4.283849   -1.3604091 ]\n",
      "models\" 83 [-0.15224403  4.784252    0.99641377 -1.2910923   2.1880217  -1.5098879 ]\n",
      "reduce 84 [-1.0707557   0.87442493  1.0498714   2.6603425   4.7372794  -0.99245894]\n",
      "high 85 [-1.0471361  -0.5423304   0.96575594  0.7723512   6.3429785  -0.24071786]\n",
      "dimensionality 86 [1.9855015  0.20320293 2.7481499  1.0878385  3.4955273  1.0616665 ]\n",
      "representations 87 [ 0.7707155   0.42355055 -0.42766008  4.5720997   1.4439294  -0.5273302 ]\n",
      "contexts 88 [ 1.1745477   1.5598025   2.3767986   4.2712502  -1.3567998   0.36881062]\n",
      "\"learning 89 [-1.2841903  2.6315608  4.8278594  0.5100992 -0.8314391  3.3099604]\n",
      "distributed 90 [ 0.31441185  4.0519595  -0.31683487  0.69481844 -1.6497545   4.26013   ]\n",
      "representation 91 [ 2.3399131   2.4456081   0.3796169   0.66034037 -1.9383959   0.84941673]\n",
      "words\" 92 [ 0.11896051 -0.03300998  0.1260847  -0.08723997 -0.13456136  0.12667535]\n",
      "(bengio 93 [ 0.68792146 -1.4048145   0.80633974  4.4387064  -1.911334    0.5541509 ]\n",
      "al, 94 [ 3.260425   -1.8808091  -1.7412771  -1.0190511   0.75997365  4.8326173 ]\n",
      "2003) 95 [ 0.02279976 -0.01737207 -0.13650078 -0.13903728 -0.0193733   0.07084581]\n",
      "come 96 [ 1.7124869   1.4837137   0.4723762   4.684141   -0.49326926 -0.91110903]\n",
      "two 97 [ 5.2669053   0.59300834  3.1737535  -1.4099016  -1.4258623  -0.17645577]\n",
      "different 98 [ 4.173622    0.61048067  0.54136395 -1.6045498  -1.120472    1.442023  ]\n",
      "styles, 99 [ 4.1792579e+00  6.8956000e-01  5.7071602e-01  9.1763914e-01\n",
      " -4.5798125e-04  1.0573590e+00]\n",
      "one 100 [1.42687   0.7183692 2.5798802 4.801088  0.5696792 0.834992 ]\n",
      "which 101 [ 2.9793549   3.4548297   2.089263   -0.27567956  1.3572071   0.35105053]\n",
      "are 102 [ 0.3093656   1.38183    -1.3297515  -1.6447643   0.16024983 -0.39543575]\n",
      "expressed 103 [ 2.0147498   3.1835005  -1.577108   -0.58519655  6.1259494   1.805676  ]\n",
      "co-occurring 104 [-1.6993191  2.3706963  1.1356888  0.5058053  2.9356506  3.3343334]\n",
      "words, 105 [-0.8951988  -1.1669585   2.7218282   4.419551    0.49737892  3.5276635 ]\n",
      "another 106 [ 2.1619565  0.7699466  1.894341   5.4127965 -0.8803712  2.9001987]\n",
      "occur; 107 [ 5.1300263   0.887073   -0.23223181 -1.0085262  -1.3018514  -1.0569165 ]\n",
      "these 108 [ 4.0953646   1.7467865   1.4030881  -0.95633966 -1.6744778   0.4788888 ]\n",
      "styles 109 [ 3.4954784  3.1636539 -0.3943835 -1.8040252  1.9593409  2.3114016]\n",
      "studied 110 [-1.4748743e-03  7.5328630e-01  4.4097546e-01  4.9081168e+00\n",
      " -9.9449748e-01  7.6361758e-01]\n",
      "(lavelli 111 [ 0.6615894 -1.4055558  0.8228173  4.371112  -1.9253308  0.5420826]\n",
      "2004) 112 [-0.01412097 -0.07149995  0.04471123  0.06098424 -0.01635051  0.02654663]\n",
      "roweis 113 [-1.3743224 -1.2803272  1.934507   1.3938638  2.1244445  5.7001967]\n",
      "saul 114 [ 2.7332485   0.06642235 -0.16136941  1.7880349  -0.18097952  2.0989957 ]\n",
      "published 115 [ 0.76654154  1.2327865   0.7248113   4.786665   -0.9311838  -0.23724337]\n",
      "science 116 [-1.5644326   1.6736116   4.049169   -0.07180936 -0.6367751  -1.7520374 ]\n",
      "how 117 [-1.6888103   2.7997756   0.75151193 -1.5604949   2.6386044  -0.82223713]\n",
      "use 118 [-1.8271985   0.51839423 -0.7755766  -1.2000834   3.7309191  -1.8505132 ]\n",
      "\"locally 119 [-1.8294145 -1.0500716 -0.6075266 -1.8075081  4.050779  -2.0117216]\n",
      "linear 120 [-1.2382667  -0.38211432  1.7798631  -1.8796954   3.416309   -1.7617683 ]\n",
      "embedding\" 121 [-1.1976403   0.55865204  2.968914   -1.885255    2.153361   -1.1757052 ]\n",
      "(lle) 122 [-1.7322198  3.5440323  1.5914669 -1.6077653  2.2760642 -1.8922018]\n",
      "discover 123 [ 1.5450221   1.1907617  -0.34496665  0.39360827  5.905131   -0.27911225]\n",
      "dimensional 124 [-1.6745714 -1.658083   2.7694662 -1.2672063  1.6114217  1.622199 ]\n",
      "structures 125 [ 0.04028897  0.05218022 -0.01536132 -0.0420787   0.08720772 -0.10150349]\n",
      "developed 126 [-0.29240987  0.12494203 -1.1691555   0.361994    1.4986418   0.18187086]\n",
      "gradually 127 [-0.07570471 -1.0065132   0.8179592   0.9106383   1.1191276   5.8291306 ]\n",
      "really 128 [-0.7938878 -1.8858032 -1.068844  -0.8471806  2.3870962  5.303271 ]\n",
      "took 129 [-1.7805718  -1.8778851  -1.4129717  -0.77530617  2.138989    3.8955264 ]\n",
      "off 130 [-1.8776037  -1.8822682  -0.41209805  0.49899614 -0.72771186  4.4537992 ]\n",
      "after 131 [-1.8509588 -1.8204426  1.4528135 -0.8758099 -1.5987787  4.4394956]\n",
      "2010, 132 [-1.8618234 -1.360126   3.5510917 -1.7191834 -1.6666019  1.7056241]\n",
      "partly 133 [-1.8635325  -1.1713902   2.0284388  -1.8876023  -0.09052475  0.59270793]\n",
      "because 134 [-1.867598   -0.28140217 -0.91227764 -1.8768009   3.4236243   0.4896128 ]\n",
      "important 135 [-1.8558365  -0.85766345 -1.9187434  -1.4006355   0.9478969   2.133413  ]\n",
      "advances 136 [-0.5635059 -1.4928544 -1.9236022  1.5505583 -0.7359271  3.889753 ]\n",
      "had 137 [ 3.713079   -0.72559357 -1.6084608  -0.09856493 -1.9268438   2.5552506 ]\n",
      "been 138 [ 2.8989198 -0.3269342  1.0019549  0.5875439 -1.9250724 -0.6388432]\n",
      "made 139 [ 3.7939315  -1.4756129   4.6220536   0.98319995 -1.8883662   0.8210653 ]\n",
      "since 140 [ 4.176261  -1.6188701 -0.1760364  4.764104  -0.5509686  3.0864954]\n",
      "quality 141 [ 1.5503793  0.4710167 -0.9881317  1.5222387  6.853099   2.021048 ]\n",
      "training 142 [ 0.10002775 -1.1419897  -0.06864723  1.8451017   2.0261288   2.2012908 ]\n",
      "speed 143 [ 0.8715462   0.8971157   2.0330033   3.5417879   3.502784   -0.19977859]\n",
      "there 144 [ 1.4428539  1.335603  -1.0082899 -0.9617406  1.8415266  4.6607995]\n",
      "many 145 [ 0.5721528  -1.3177291  -0.9447263   0.16127583 -0.47467884 -0.30621424]\n",
      "branches 146 [-0.44842353  0.8370958  -0.6256367  -0.06087337  1.7227426   5.5275693 ]\n",
      "groups 147 [ 1.4566561 -1.5264933 -1.5246689  3.8593276  1.197906   2.7827766]\n",
      "working 148 [ 1.8756534  -0.80716395  0.09692323  0.7376083   1.1978621   4.0836315 ]\n",
      "2013, 149 [-1.3383554   0.32054526  4.7043424   0.8566232  -0.76100004  2.6256979 ]\n",
      "team 150 [-1.5735909 -0.5288932  1.8949534  4.1422567 -1.9348334 -1.3445921]\n",
      "at 151 [ 2.9348798 -0.7763889  0.3616855  3.0766947 -1.6621401 -1.4641801]\n",
      "google 152 [ 3.910623   -0.00461434  2.0038745   2.9702685  -1.2150629  -1.0987375 ]\n",
      "tomas 153 [ 0.45544517 -1.3415152   4.107923   -0.76129466 -1.7745339  -1.3599263 ]\n",
      "mikolov 154 [ 3.0666988 -1.8576835  3.6518998 -1.523687  -1.3019388 -1.3463684]\n",
      "created 155 [ 0.5285806  -1.0492688   3.6311538  -0.79620844 -1.1366875   0.6203785 ]\n",
      "word2vec, 156 [-1.3622794  -0.5458794   0.97288203 -1.7248068  -0.8305501   0.47569257]\n",
      "embedding 157 [-1.0041541  -0.25084442 -0.72552705 -1.0394777  -1.1496707   3.5079908 ]\n",
      "toolkit 158 [ 2.3869834   0.24230397  2.4266214   0.7391798  -1.1390008   2.7740521 ]\n",
      "can 159 [ 0.4061803   4.812125    1.2862304  -0.15068035 -1.4809227  -0.74666995]\n",
      "train 160 [-1.3648452   2.764365   -0.62407786  2.9683504   0.55399805 -0.19182554]\n",
      "models 161 [-1.7951596  1.3406601 -0.5625305  2.2982316 -0.9196111  1.7986416]\n",
      "faster 162 [-1.8568189   4.1827073   3.5255218   3.2317085   1.2757478   0.11266783]\n",
      "than 163 [-0.4919592  1.1229988  1.1442343  2.812025   3.3289845 -1.655493 ]\n",
      "previous 164 [ 1.8528373 -1.6059222 -1.9385537  3.7256732  4.2625365 -2.0119748]\n",
      "approaches 165 [ 0.14193214 -0.07361897  0.02229138 -0.0355548   0.09314333  0.00341581]\n",
      "most 166 [-0.6786682  -1.7444918   4.835296    0.24071385  2.2684674   2.3935049 ]\n",
      "new 167 [ 1.2625009  -0.9048503   3.5262115  -1.0369971  -0.47205833  3.3827415 ]\n",
      "techniques 168 [-0.74821335 -0.6790601  -1.3631583   4.616628   -0.60049736  4.5693326 ]\n",
      "rely 169 [ 0.8665454  -0.59953433 -0.25642505  1.2344637   0.16477604  2.5708907 ]\n",
      "neural 170 [-1.8504157  -1.2933012   4.9282217  -0.49778095 -1.6184732  -1.58641   ]\n",
      "network 171 [-1.8633355  -0.41033253  1.0151637  -0.15536453  2.0885668  -2.0217786 ]\n",
      "architecture 172 [ 0.6075862  -0.14444572  1.272974    1.102656    4.9334445  -1.5669805 ]\n",
      "instead 173 [ 0.50320876 -0.4718104  -0.08933163  0.43131214  5.6729693   1.3977958 ]\n",
      "more 174 [-0.44101158 -0.84419423 -1.5485841  -1.5365654   4.8446436  -0.47279584]\n",
      "traditional 175 [-0.6804825  -0.6465572  -1.9348975  -0.76051426  5.1093073   1.0431439 ]\n",
      "n-gram 176 [-1.2943074   1.808557   -0.91758126  2.8196342   2.481886    5.840079  ]\n",
      "unsupervised 177 [ 0.20072755 -0.82103467 -0.93660223  1.8192717   3.7936926   4.548078  ]\n",
      "learning 178 [-0.53346825  2.9335086   0.20832865  5.097624   -0.98415685 -0.02344025]\n",
      "limitations 179 [ 3.531597   -0.63879156  2.165867    0.6469651   1.5958263   1.651161  ]\n",
      "main 180 [ 2.8060136  -1.5229566  -0.88071585  4.748491    2.2902064  -0.527879  ]\n",
      "(word 181 [-1.3628936   2.7725644  -0.6217639   2.9807227   0.5572205  -0.18873452]\n",
      "general) 182 [ 2.9626434  -0.48327324  3.6956365   0.03279193  1.0232948  -1.1689906 ]\n",
      "possible 183 [ 4.097899   -0.71883774 -0.5905643   0.17165266  2.7992058   1.1799341 ]\n",
      "meanings 184 [1.6712804 0.2528041 2.5708468 1.9629517 1.8488575 0.9062323]\n",
      "conflated 185 [-0.65302193  1.6491587  -0.5980548  -1.2265527   0.16418493  0.28317794]\n",
      "into 186 [ 0.8856711  2.6962972  5.961171   1.2485353 -1.3498981  3.587775 ]\n",
      "single 187 [ 0.15233374  4.0194435   0.8675261   3.4092693  -1.6825161   1.5846897 ]\n",
      "(a 188 [ 0.5875763  2.6718922  2.2653706  1.6958508 -1.562608   1.6612571]\n",
      "space) 189 [ 0.07901341 -0.03048456  0.02950275 -0.00859366  0.12117656 -0.06517252]\n",
      "sense 190 [ 1.6326241   2.7690022   0.89921784 -1.3086857   2.3124328   3.939451  ]\n",
      "solution 191 [-1.679049    4.035259    0.96974653 -0.8604395  -0.5358435  -0.5233184 ]\n",
      "this 192 [-1.0510162  2.703593  -1.71943   -1.4481201 -1.7720168  1.867858 ]\n",
      "problem: 193 [ 3.5405314  -0.39934266 -1.8643368  -1.4656936   0.282043    1.8169999 ]\n",
      "individual 194 [ 4.130091   -0.71646917 -0.58840185  0.16453125  2.7913089   1.1740983 ]\n",
      "represented 195 [ 3.7488513   2.1736887  -1.9223261  -0.93657565  3.1492815  -0.14267984]\n",
      "distinct 196 [ 2.3594508  4.181692  -1.510138   1.8809314  2.4808254  2.2424223]\n",
      "biological 197 [ 3.725803   4.7034407 -0.4857329  1.0702953  1.1511532 -1.6042395]\n",
      "sequences: 198 [ 1.7986712   0.7295129   1.9382637  -1.0028789   2.8126261  -0.39549848]\n",
      "biovectors 199 [ 1.7535573  1.4354885  2.9538832 -1.1516571 -0.4793949  1.3895215]\n",
      "n-grams 200 [ 0.51554143  4.4514503   0.01681666  3.6754992  -0.7352887  -1.0280267 ]\n",
      "sequences 201 [ 3.1195288   2.0423431  -0.24973877  3.2159154  -0.43691528 -1.337869  ]\n",
      "(e 202 [-0.03613399  0.0580198   0.02752756 -0.08513927 -0.08308613 -0.02407567]\n",
      "g 203 [ 0.08127129  0.10882472  0.05619447 -0.07724605 -0.02654643 -0.01011364]\n",
      "dna, 204 [-1.8605076   0.20790339  0.8580331   0.25616583  1.0653573   4.9722266 ]\n",
      "rna, 205 [-1.0486443  -0.72848547  0.74505866  4.155081   -0.70258164  5.254126  ]\n",
      "proteins) 206 [ 3.3234663   4.30594    -1.6135784  -0.01661145 -1.4432403   0.79488873]\n",
      "bioinformatics 207 [ 1.5234846   0.4605727  -1.7127352   1.1483753  -0.65173036 -1.2044693 ]\n",
      "applications 208 [ 1.1028953  -0.9312772  -1.8428187   1.2606745   0.30414355  0.19380479]\n",
      "have 209 [ 4.1108365  -1.4001502  -1.8575813   0.50726545 -1.8930452   0.0166749 ]\n",
      "proposed 210 [-0.63978356  2.8025792   4.313328    2.8522978  -1.8218298  -1.0740772 ]\n",
      "asgari 211 [-1.1749985 -1.1804571  3.7300165  4.989669   0.5852487  7.013264 ]\n",
      "mofrad 212 [ 4.2367663  -1.8754493   2.8907723  -0.12111565  1.146037   -1.1977675 ]\n",
      "named 213 [-1.8469892   1.8124803  -0.76874024 -1.8968409  -1.4838175   3.7958796 ]\n",
      "bio-vectors 214 [-1.7594861  3.5500255  2.1636188 -1.8767753  0.2791125  0.8195032]\n",
      "(biovec) 215 [-1.8484708   4.1124334   1.3329966  -1.6707945  -0.18063372 -2.0124397 ]\n",
      "refer 216 [-0.84397906  4.810002    2.374495   -0.14887343 -0.69172186 -1.6348522 ]\n",
      "general 217 [-1.8520216   0.49734136 -0.68750674  4.8896737  -0.9396202  -2.0031216 ]\n",
      "protein-vectors 218 [ 0.5031186   4.139583   -1.7609766   0.529243   -0.64277905  0.9246386 ]\n",
      "(protvec) 219 [ 0.09563296  3.920954   -1.9219575   1.8548024  -1.4411491   3.3225577 ]\n",
      "proteins 220 [-1.744759    4.44132    -1.9318393   3.0473974   0.9501409  -0.92608637]\n",
      "(amino-acid 221 [-1.8553288  2.0755813 -0.7092974  2.7729664  1.0870383  1.5782559]\n",
      "sequences) 222 [-1.4831679  -1.0592766   2.1052225   0.93540037  0.52094793  5.6173425 ]\n",
      "gene-vectors 223 [ 2.7148917   0.04959653  0.11283787  0.7603482  -1.3920126   5.6913676 ]\n",
      "(genevec) 224 [ 3.7750747   2.3778875  -1.2987636   0.85269725 -1.702169    1.0879098 ]\n",
      "gene 225 [-1.6164564  4.5753455 -1.6705397  1.1140574 -1.683714  -1.7201054]\n",
      "sequences, 226 [-1.3687885   4.020933   -0.15660144  0.26823875 -1.7314966   0.7908311 ]\n",
      "be 227 [ 3.1319542  5.232603   1.9562193 -1.7262204 -1.1190574 -2.0078182]\n",
      "widely 228 [ 3.1487439   4.841788    0.39413482 -0.7870431   0.29573146 -0.46546358]\n",
      "used 229 [ 0.47353607  4.8040347   0.55573565 -0.6043818   0.23564343 -0.81432945]\n",
      "deep 230 [ 0.5260472  0.1726738 -0.9120685  1.8743368  1.5463041  2.530166 ]\n",
      "proteomics 231 [-1.082096   -0.94103193  0.66353124  1.947762    0.13317579  5.212103  ]\n",
      "genomics 232 [ 0.13117962  0.10991932 -0.13667087  0.01336087 -0.09876471 -0.09012175]\n",
      "results 233 [ 4.6476445   1.5662097   1.1157631   2.6601963  -0.68197006 -2.0031598 ]\n",
      "presented 234 [-0.6409228  2.7864122  4.284117   2.8257155 -1.8227853 -1.0794448]\n",
      "suggest 235 [ 2.406038    1.6681933   0.68642837 -0.9015621   2.854525   -1.3187732 ]\n",
      "characterize 236 [ 0.02952227  4.8506875   1.044251    1.773336   -0.22735167 -2.007531  ]\n",
      "terms 237 [ 0.22680834 -0.60316235 -0.27192745  0.95753706  5.5101953   1.5676754 ]\n",
      "biochemical 238 [-0.91093355 -0.74923503  0.18380874  1.5578573   0.04391773  5.1730733 ]\n",
      "biophysical 239 [ 2.1446378 -1.8744501  0.6660512  4.950181   1.4294279  1.6534604]\n",
      "interpretations 240 [ 0.83711684  0.8738743   2.0480084   3.5544095   3.447834   -0.21313064]\n",
      "patterns 241 [-0.01291728  0.10629687  0.03657955 -0.07828292  0.06380697 -0.09951285]\n",
      "thought 242 [ 0.9104299   4.8992558  -1.3861363   0.06280704  3.0376244   3.850459  ]\n",
      "an 243 [ 3.9755576   0.42072535 -1.1252807   0.6029728   3.0016894  -0.2626824 ]\n",
      "extension 244 [ 1.6292893  -0.61389863  2.445099    0.8903192   3.326932    2.122023  ]\n",
      "entire 245 [-1.7564422   2.0311594  -1.677827    0.48140934  4.1666594  -1.9803694 ]\n",
      "sentences 246 [-1.8482788 -0.4110169 -1.9149213  2.7905738  2.5244095 -1.9924661]\n",
      "or 247 [-1.8597555  -1.5513743  -1.9314057   0.65504897  2.7480075  -1.5872087 ]\n",
      "even 248 [-1.8628272  -1.8094316  -1.9233085   0.0382506   4.211312   -0.40758044]\n",
      "documents 249 [ 0.00910807 -0.03670707  0.0549929  -0.07077391  0.00471231 -0.11768984]\n",
      "some 250 [ 0.21021473 -1.2409842  -1.8574728  -1.398471    4.46784    -2.0136735 ]\n",
      "researchers 251 [ 2.1279252  -0.6423324  -0.43087408 -1.4724115   2.7927816  -2.0177968 ]\n",
      "hope 252 [ 5.971085   0.9229864 -0.540786  -1.3995632  0.8365961 -1.2665614]\n",
      "improve 253 [-0.5834501   0.35109538  1.3600665   1.8124338   3.441913   -1.1146948 ]\n",
      "machine 254 [-1.8761773  -0.8388453   0.42480847  3.4073622   4.5186186   3.8657293 ]\n",
      "translation 255 [-0.10616106  0.02495779 -0.00503394 -0.04022801  0.10318755 -0.07799426]\n",
      "software 256 [ 0.86221397  0.7989276  -1.2200835   1.5147246  -1.5266353   2.6543925 ]\n",
      "includes 257 [-0.1919862   0.29168847  2.8218563  -1.0924339  -1.8949699  -1.5721178 ]\n",
      "mikolov's 258 [ 0.7266645  -1.545821    2.6186693  -1.9001726  -0.94822174 -0.07711759]\n",
      "stanford 259 [-1.7924343   2.6034157   0.34326494 -1.778172   -1.6453868  -0.1975273 ]\n",
      "university's 260 [-1.8563906   1.9851261  -0.695346   -0.21287161 -1.9349129  -0.71352714]\n",
      "glove, 261 [-1.7814729  -0.89279544 -1.8731271   1.9461691  -1.9447907   1.1289096 ]\n",
      "allennlp's 262 [-0.24031292 -1.505924   -1.7164487  -1.7733402  -1.9521871   4.933615  ]\n",
      "elmo,fasttext, 263 [ 3.5798645 -1.2526575 -0.5046723 -1.5333232 -1.3965526  4.13182  ]\n",
      "gensim, 264 [ 0.330625   -0.8557212   0.91819036 -0.225958   -0.8147361   4.9346642 ]\n",
      "indra 265 [-1.3580214  -0.93193555  0.05303869  4.786511    0.5359204   5.010693  ]\n",
      "deeplearning4j 266 [ 0.06304769 -0.11717276 -0.03213556 -0.01498447 -0.05281016  0.13003053]\n",
      "principal 267 [ 1.6816587  -1.8804735   1.6015111  -1.8398417   3.6363075   0.40019882]\n",
      "component 268 [ 4.3813667 -1.1221733  3.0860972 -1.1403251  2.7045178  1.1029946]\n",
      "(pca) 269 [-1.1007189 -1.2171594  1.1918238  4.5344453  1.4956493  4.1896505]\n",
      "t-distributed 270 [ 4.594243   -1.8909129  -0.26308188 -1.4806795  -0.44443354  0.44657704]\n",
      "stochastic 271 [ 1.9299093  -1.253859    0.22969131 -1.8864602  -0.6256598   1.4933566 ]\n",
      "neighbour 272 [-0.987971  -0.9807945  0.6507921 -1.8798212 -1.521834   4.829522 ]\n",
      "(t-sne) 273 [ 3.6737936   3.5275695  -0.98311234 -1.7232398   0.86668605  4.6939807 ]\n",
      "both 274 [ 1.1978236  4.749978   2.570828  -1.7469472  0.8542388 -1.0589801]\n",
      "spaces 275 [-0.4840236 -0.5867031  1.4833719  1.3191161 -0.8755026  5.521993 ]\n",
      "visualize 276 [ 0.7318418   0.0129352   3.5853689  -0.88753986  3.1556168   2.9310741 ]\n",
      "clusters 277 [-0.13787709 -0.11572106 -0.08929805  0.00093365  0.04825164  0.06654188]\n",
      "examples 278 [ 0.31285426 -0.59902155 -0.35551074  0.74291945  5.655489    1.0743699 ]\n",
      "application 279 [-0.29669     4.078554   -1.4250909   0.99936986 -1.4887719   1.2458261 ]\n",
      "instance, 280 [ 0.06495576  1.6238012   2.174866    3.7113183   4.148494   -1.9840901 ]\n",
      "fasttext 281 [ 2.1875317  -0.5081623   5.6698666  -0.58543533  0.63397855 -1.1665426 ]\n",
      "also 282 [ 1.2073262   4.7224693   2.5688236  -1.7478006   0.85162646 -1.063059  ]\n",
      "calculate 283 [ 0.74623644  0.03570469  3.6062105  -0.8716523   3.1954088   2.9648576 ]\n",
      "text 284 [ 1.1083938   4.2956777  -1.8285582   4.751915   -0.00899996  0.5764814 ]\n",
      "corpora 285 [ 1.078455    0.779054    0.8945711   5.0183754  -1.0918479   0.27984023]\n",
      "sketch 286 [ 1.4860893  -0.68693334  3.285673    0.21103711  2.048138   -1.9145039 ]\n",
      "engine 287 [ 3.1398625   1.388847    0.46397182 -1.0013769   1.8969288  -0.6158497 ]\n",
      "available 288 [ 0.6608625  4.220581  -1.7306898 -1.8983806  1.4962322 -2.0165186]\n",
      "online 289 [-0.1356279  -0.05663224 -0.0357901   0.06276853 -0.00373261  0.06934156]\n"
     ]
    }
   ],
   "source": [
    "for k,v in WordToInd.items():\n",
    "    print (k,v, wts[0][v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expressed [ 2.0147498   3.1835005  -1.577108   -0.58519655  6.1259494   1.805676  ]\n",
      "proposed [-0.63978356  2.8025792   4.313328    2.8522978  -1.8218298  -1.0740772 ]\n",
      "semantics [ 0.10100773 -0.05692254  0.13033839 -0.03017608  0.10222936 -0.12685558]\n",
      "semantic [ 0.9327205  -1.8306663  -1.0781221  -0.41344702  1.7052569  -0.37389767]\n"
     ]
    }
   ],
   "source": [
    "print (\"expressed\", wts[0][WordToInd[\"expressed\"]])\n",
    "print ( \"proposed\", wts[0][WordToInd[\"proposed\"]])\n",
    "print ( \"semantics\", wts[0][WordToInd[\"semantics\"]])\n",
    "print ( \"semantic\", wts[0][WordToInd[\"semantic\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
